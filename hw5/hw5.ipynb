{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M280 Homework5\n",
    "* Author: Shuang Gao\n",
    "* Date: 2018/06/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the MLE of the Dirichlet-multinomial model. In [HW4](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html), we worked out a Newton's method. In this homework, we explore the MM and EM approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Show that, given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, the log-likelihood can be written as\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "Hint: $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Solution**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\because \\mathbf{x_1},...,\\mathbf{x_n} \\text{ are independent}\\\\ \n",
    "& \\therefore f(\\mathbf{x_1},...,\\mathbf{x_n} \\mid \\alpha) \n",
    "= f(\\mathbf{x_1}\\mid \\alpha)...f(\\mathbf{x_n\\mid \\alpha})\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\therefore f(\\mathbf{x_1},...,\\mathbf{x_n} \\mid \\alpha) \n",
    "= \\prod_{i=1}^n \\binom{|\\mathbf{x_i}|}{\\mathbf{x_i}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_{ij})}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x_i}|)}\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\therefore L(\\alpha) \n",
    "&= \\ln (\\prod_{i=1}^n \\binom{|\\mathbf{x_i}|}{\\mathbf{x_i}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_{ij})}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x_i}|)})\\\\\n",
    "&= \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)]\\\\\n",
    "&= \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d \\ln \\frac{\\Gamma(\\alpha_j+x_{ij})}{\\Gamma(\\alpha_j)} -\\sum_{i=1}^n \\ln \\frac{\\Gamma(|\\alpha|+|x_i|)}{\\Gamma(|\\alpha|)}\\\\\n",
    "&= \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d\\ln [\\prod_{k=0}^{x_{ij}-1}(\\alpha_j+k)]-\\sum_{i=1}^n\\ln[\\prod_{k=0}^{|\\mathbf{x_i}|-1}(|\\alpha|+k)] \\\\\n",
    "&=  \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k)-\\sum_{i=1}^n\\sum_{k=0}^{|\\mathbf{x_i}|-1}\\ln(|\\alpha|+k)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Suppose $(P_1,\\ldots,P_d) \\in \\Delta_d = \\{\\mathbf{p}: p_i \\ge 0, \\sum_i p_i = 1\\}$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1,\\ldots,\\alpha_d)$. Show that\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$\n",
    "where $\\Psi(z) = \\Gamma'(z)/\\Gamma(z)$ is the digamma function and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$. Hint: Differentiate the identity \n",
    "$$\n",
    "1 = \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Solution**\n",
    "Differentiate the identity with respect to $\\alpha_j$,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{d\\alpha_j}1 &= \\frac{d}{d\\alpha_j}\\int_{\\mathbf{p}_1}^{{\\mathbf{p}_2}} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\\\\\n",
    "0 &= \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d \\mathbf{p_2}_j^{\\alpha_j-1} \\frac{d}{d\\alpha_j}\\mathbf{p}_2\n",
    "-\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d \\mathbf{p_1}_j^{\\alpha_j-1} \\frac{d}{d\\alpha_j}\\mathbf{p}_1+\\int_{\\mathbf{p}_1}^{{\\mathbf{p}_2}}\\frac{d}{d\\alpha_j}[ \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1}] \\, d\\mathbf{p}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\because \\frac{d}{d\\alpha_j}\\mathbf{p}_2=0 \\,\\frac{d}{d\\alpha_j}\\mathbf{p}_1=0\\\\\n",
    "\\therefore \\int_{\\mathbf{p}_1}^{{\\mathbf{p}_2}}\\frac{d}{d\\alpha_j}[ \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1}] \\, d\\mathbf{p}=0 \n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    " \\int_{\\mathbf{p}_1}^{{\\mathbf{p}_2}}[\\prod_{i=1}^d p_i^{\\alpha_i - 1}\\frac{\\Gamma(|\\alpha|)'}{\\prod_{i=1}^{d}\\Gamma(\\alpha_i)}-\\frac{\\Gamma(|\\alpha|)}{\\prod_{i\\neq j}^d\\Gamma(\\alpha_i)}\\frac{\\Gamma(\\alpha_j)'}{\\Gamma(\\alpha_j)}^2\\prod_{i=1}^dp_i^{\\alpha_i-1}+\\frac{\\Gamma(|\\alpha|)}{\\prod_{i= 1}^d\\Gamma(\\alpha_i)}\\prod_{i\\neq j}^d p_i^{\\alpha_i-1}\\ln(p_j) p_j^{\\alpha_j-1}]\\, d\\mathbf{p}\\, d\\mathbf{p}=0\\\\\n",
    "\\int_{\\mathbf{p}_1}^{{\\mathbf{p}_2}}\\prod_{i=1}^d p_i^{\\alpha_i - 1}\\frac{\\Gamma(|\\alpha|)'}{\\prod_{i=1}^{d}\\Gamma(\\alpha_i)}\\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|)}\\, d\\mathbf{p}-\\int_{\\mathbf{p}_1}^{{\\mathbf{p}_2}}\\frac{\\Gamma(|\\alpha|)}{\\prod_{i=1}^d\\Gamma(\\alpha_i)}\\frac{\\Gamma(\\alpha_j)'}{\\Gamma(\\alpha_j)}\\prod_{i=1}^dp_i^{\\alpha_i-1}\\, d\\mathbf{p}+\\int_{\\mathbf{p}_1}^{{\\mathbf{p}_2}}\\frac{\\Gamma(|\\alpha|)}{\\prod_{i= 1}^d\\Gamma(\\alpha_i)}\\prod_{i=1}^d p_i^{\\alpha_i-1}\\ln(p_j)\\, d\\mathbf{p}=0\\\\\n",
    "\\frac{\\Gamma(|\\alpha|)'}{\\Gamma(|\\alpha|)}-\\frac{\\Gamma(\\alpha_j)'}{\\Gamma(\\alpha_j)}+\\mathbf{E}(\\ln p_j)=0\\\\\n",
    "\\therefore \\mathbf{E}(\\ln p_j)=\\Psi(\\alpha_j) - \\Psi(|\\alpha|)\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "The admixture representation of the Dirichlet-multinomial distribution suggests that we can treat the unobserved multinomial parameters $\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ as missing data and derive an EM algorithm. Show that the Q function is\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$\n",
    "where $c^{(t)}$ is a constant irrelevant to optimization. Comment on why it is not easy to maximize the Q function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Solution**\n",
    "\n",
    "$\\mathbf{p_1},..., \\mathbf{p_n}$ are independent Dirichlet distribution, therefore we have \n",
    "$$\n",
    "f(\\mathbf{p_i}|\\mathbf{x_1},...,\\mathbf{x_n},\\mathbf{\\alpha}^{(t)})=\\frac{\\Gamma(|\\alpha_{(t)}+|\\mathbf{x_i}|)}{\\prod_{j=1}^d\\Gamma(\\alpha_j^{(t)}+\\mathbf{x}_{ij})}\\prod_{j=1}^{d}p_{ij}^{x_{ij}+\\alpha_j^{(t)}-1},\n",
    "$$\n",
    "$$\n",
    "\\int_{\\Delta d}f(\\mathbf{p_i}|\\mathbf{x_1},...,\\mathbf{x_n},\\mathbf{\\alpha}^{(t)}) d\\mathbf{p_i}=\\frac{\\Gamma(|\\alpha_{(t)}+|\\mathbf{x_i}|)}{\\prod_{j=1}^d\\Gamma(\\alpha_j^{(t)}+\\mathbf{x}_{ij})}\\prod_{j=1}^{d}p_{ij}^{x_{ij}+\\alpha_j^{(t)}-1} d\\mathbf{p_i}=1,\n",
    "$$and from question 2,\n",
    "$$\n",
    "\\mathbf{E}(\\ln P_{ij}) = \\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|).\n",
    "$$\n",
    "Then we have,\n",
    "$$\n",
    "\\begin{align}\n",
    " Q(\\alpha|\\alpha^{(t)}) \n",
    " &= \\mathbf{E}[\\ln\\prod_{i=1}^n f(\\mathbf{x_i},\\mathbf{p_i},\\alpha)|\\mathbf{x_1},...,\\mathbf{x_n},\\mathbf{\\alpha}^{(t)}]\\\\\n",
    " &= \\sum_{i=1}^n\\mathbf{E}[\\ln f(\\mathbf{x_i},\\mathbf{p_i},\\alpha)|\\mathbf{x_1},...,\\mathbf{x_n},\\mathbf{\\alpha}^{(t)}]\\\\\n",
    " &= \\sum_{i=1}^n  Q_i(\\alpha|\\alpha^{(t)}) \n",
    "\\end{align}\n",
    "$$ with respect to $f(\\mathbf{p_i}|\\mathbf{x_1},...,\\mathbf{x_n},\\mathbf{\\alpha}^{(t)})$\n",
    "\n",
    "Therefore we have,\n",
    "$$\n",
    "\\begin{align}\n",
    " Q_i(\\alpha|\\alpha^{(t)}) \n",
    " &= \\int_{\\Delta d}f(\\mathbf{p_i}|\\mathbf{x_1},...,\\mathbf{x_n},\\mathbf{\\alpha}^{(t)})\\ln f(\\mathbf{x_i},\\mathbf{p_i},\\alpha)d\\mathbf{p_i}\\\\\n",
    " &= \\int_{\\Delta d}f(\\mathbf{p_i}|\\mathbf{x_1},...,\\mathbf{x_n},\\mathbf{\\alpha}^{(t)})\\ln [f(\\mathbf{x_i}|\\mathbf{p_i})\\mathbf{\\pi}(\\mathbf{p_i})]d\\mathbf{p_i}\\\\\n",
    " &= \\int_{\\Delta d}\\frac{\\Gamma(|\\alpha_{(t)}+|\\mathbf{x_i}|)}{\\prod_{j=1}^d\\Gamma(\\alpha_j^{(t)}+\\mathbf{x}_{ij})}\\prod_{j=1}^{d}p_{ij}^{x_{ij}+\\alpha_j^{(t)}-1}\\ln [ \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\\prod_{j=1}^dp_{ij}^{x_ij}\\frac{\\Gamma(|\\alpha^{(t)}|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j^{(t)})} \\prod_{j=1}^d p_j^{\\alpha_j^{(t)}-1}]d\\mathbf{p_i}\\\\\n",
    " &=  \\int_{\\Delta d}\\frac{\\Gamma(|\\alpha_{(t)}+|\\mathbf{x_i}|)}{\\prod_{j=1}^d\\Gamma(\\alpha_j^{(t)}+\\mathbf{x}_{ij})}\\prod_{j=1}^{d}p_{ij}^{x_{ij}+\\alpha_j^{(t)}-1}[\\ln\\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}+\\sum_{j=1}^d (x_{ij}\\ln p_{ij})+\\ln \\Gamma(|\\alpha^{(t)}|)-\\sum_{j=1}^{d}\\ln\\Gamma(\\alpha_j^{(t)})+\\sum_{j=1}^{d}(\\alpha_j-1)\\ln p_{ij}]d\\mathbf{p_i}\\\\\n",
    " &=  \\int_{\\Delta d}\\frac{\\Gamma(|\\alpha_{(t)}+|\\mathbf{x_i}|)}{\\prod_{j=1}^d\\Gamma(\\alpha_j^{(t)}+\\mathbf{x}_{ij})}\\prod_{j=1}^{d}p_{ij}^{x_{ij}+\\alpha_j^{(t)}-1}[\\ln\\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}+\\sum_{j=1}^d (x_{ij}+\\alpha_j^{(t)}-1)\\ln p_{ij}+\\ln \\Gamma(|\\alpha^{(t)}|)-\\sum_{j=1}^{d}\\ln\\Gamma(\\alpha_j^{(t)})]d\\mathbf{p_i}\\\\\n",
    " &= \\ln\\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}+\\ln \\Gamma(|\\alpha^{(t)}|)-\\sum_{j=1}^{d}\\ln\\Gamma(\\alpha_j^{(t)})+ \\int_{\\Delta d}\\frac{\\Gamma(|\\alpha_{(t)}+|\\mathbf{x_i}|)}{\\prod_{j=1}^d\\Gamma(\\alpha_j^{(t)}+\\mathbf{x}_{ij})}\\ln p_{ij}\\prod_{j=1}^{d}p_{ij}^{x_{ij}+\\alpha_j^{(t)}-1}d\\mathbf{p_i}\\sum_{j=1}^{d}(x_{ij}+\\alpha_j^{(t)}-1)\\\\\n",
    " &= \\ln\\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}+\\ln \\Gamma(|\\alpha^{(t)}|)-\\sum_{j=1}^{d}\\ln\\Gamma(\\alpha_j^{(t)})+\\sum_{j=1}^d(x_{ij}+\\alpha_j^{(t)}-1)\\mathbf{E}(\\ln p_{ij})\\\\\n",
    " &= \\ln\\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}+\\ln \\Gamma(|\\alpha^{(t)}|)-\\sum_{j=1}^{d}\\ln\\Gamma(\\alpha_j^{(t)})+\\sum_{j=1}^d(x_{ij}+\\alpha_j^{(t)}-1)\\{\\Psi(x_{ij}+\\alpha_j^{(t)})-\\Psi(|x_{i}|+|\\alpha^{(t)}|)\\}\n",
    "\\end{align}\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "\\begin{align}\n",
    "Q(\\alpha|\\alpha^{(t)}) \n",
    "&= \\sum_{i=1}^nQ_i(\\alpha|\\alpha^{(t)})\\\\ \n",
    "&=\\sum_{i=1}^n\\{ \\ln\\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}+\\ln \\Gamma(|\\alpha^{(t)}|)-\\sum_{j=1}^{d}\\ln\\Gamma(\\alpha_j^{(t)})+\\sum_{j=1}^d(x_{ij}+\\alpha_j^{(t)}-1)\\{\\Psi(x_{ij}+\\alpha_j^{(t)})-\\Psi(|x_{i}|+|\\alpha^{(t)}|)\\}  \\}\\\\\n",
    "&= \\sum_{i=1}^n \\ln\\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}+n\\ln \\Gamma(|\\alpha^{(t)}|)-n\\sum_{j=1}^{d}\\ln\\Gamma(\\alpha_j^{(t)})+\\sum_{i=1}^n\\sum_{j=1}^d(x_{ij}+\\alpha_j^{(t)}-1)\\{\\Psi(x_{ij}+\\alpha_j^{(t)})-\\Psi(|x_{i}|+|\\alpha^{(t)}|)\\}\\\\\n",
    "&=\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)}\n",
    "\\end{align}\n",
    "$$\n",
    "where $c^{(t)}$ is a constant irrelevant to optimization.\n",
    "\n",
    "* It is not easy to maximize the Q function. Because we want to maximize the Q function with the parameter $\\alpha_j$. But in term $\\ln \\Gamma(|\\alpha|)$, it contains the $\\alpha_j$ term as a part of sum in $|\\alpha|$. Thus, it is hard to separate $\\alpha_j$ out and calculate independently. This makes maximing the Q function hard to approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "We derive an MM algorithm for maximing $L$. Consider the formulation of the log-likelihood that contains terms $\\ln (\\alpha_j + k)$ and $- \\ln (|\\alpha|+k)$. Applying Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$ and supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$, show that a minorizing function to $L(\\alpha)$ is\n",
    "$$\n",
    "\tg(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$\n",
    "where $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, and  $c^{(t)}$ is a constant irrelevant to optimization. Maximizing the surrogate function $g(\\alpha|\\alpha^{(t)})$ is trivial since $\\alpha_j$ are separated. Show that the MM updates are\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$\n",
    "The quantities $s_{jk}$, $r_k$, $\\max_i x_{ij}$ and $\\max_i |\\mathbf{x}_i|$ only depend on data and can be pre-computed. Comment on whether the MM updates respect the parameter constraint $\\alpha_j>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Solution**\n",
    "From Question 1, the log-likelihood can be written as\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "We can write the second term as\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k)\n",
    "&=\\sum_{i=1}^n\\sum_{j=1}^d\\sum_{k=0}^{\\max_i x_{ij}-1} \\ln (\\alpha_j +k)1_{\\{x_{ij}-1>k\\}}\\\\\n",
    "&=\\sum_{j=1}^d\\sum_{k=0}^{\\max_i x_{ij}-1}\\ln (\\alpha_j +k) \\sum_{i=1}^n1_{\\{x_{ij}-1>k\\}}\\\\\n",
    "&=\\sum_{j=1}^d\\sum_{k=0}^{\\max_i x_{ij}-1}\\ln (\\alpha_j +k) s_{jk}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "We can write the thrid term as\n",
    "$$\n",
    "\\begin{align}\n",
    " - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k)\n",
    " &= - \\sum_{i=1}^n\\sum_{k=0}^{\\max_i |\\mathbf{x_i}|-1} \\ln(|\\alpha|+k)1_{\\{|\\mathbf{x}_i|-1\\}}\\\\\n",
    " &= -\\sum_{k=0}^{\\max_i |\\mathbf{x_i}|-1} \\ln(|\\alpha|+k) \\sum_{i=1}^n1_{\\{|\\mathbf{x}_i|-1\\}}\\\\\n",
    "  &= -\\sum_{k=0}^{\\max_i |\\mathbf{x_i}|-1} \\ln(|\\alpha|+k) r_k\n",
    "\\end{align}\n",
    "$$\n",
    "* Apply Jensen's inequality to the concanve term $\\ln (\\alpha_j +k)$\n",
    "\n",
    "For concave function $g()$, $g(tx, (1-t)y)\\geq tg(x)+(1-t)g(y)$ by Jensen's inequality. In our case, $ln()$ is a concave function, therefore we get\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ln (\\alpha_j +k) &= \\ln(\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}\\frac{\\alpha_j^{(t)}+k}{\\alpha_j^{(t)}}\\alpha_j+\\frac{k}{\\alpha_j^{(t)}+k}\\frac{\\alpha_j^{(t)}+k}{k}k)\\\\\n",
    "&\\geq \\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln(\\frac{\\alpha_j^{(t)}+k}{\\alpha_j^{(t)}}\\alpha_j)+\\frac{k}{\\alpha_j^{(t)}}\\ln (\\frac{\\alpha_j^{(t)}+k}{k}k)\\\\\n",
    "&=\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}  [\\ln(\\alpha_j^{(t)}+k)-\\ln(\\alpha_j^{(t)})+\\ln(\\alpha_j) ]+(1-\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k})[\\ln(\\alpha_j^{(t)}+k)-\\ln(k)+\\ln(k)]\\\\\n",
    "&= \\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln(\\alpha_j)-\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln(\\alpha_j^{(t)})+\\ln(\\alpha_j^{(t)}+k)\\\\\n",
    "&= \\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln(\\alpha_j)+c^{(t)}\n",
    "\\end{align}\n",
    "$$\n",
    "where $c^{(t)}$ is a constant term.\n",
    "\n",
    "* Apply hyperplane inequality to the convex term $ -\\ln(|\\alpha|+k) $\n",
    "\n",
    "From the supporting hyperplane property of a convex function $h(x)=-\\ln x$, we have $-\\ln y \\geq -\\ln x-\\frac{1}{x}(y-x)$. Then we have \n",
    "$$\n",
    "\\begin{align}\n",
    "-\\ln(|\\alpha|+k) \n",
    "&\\geq - \\frac{|\\alpha|-|\\alpha^{(t)}|}{|\\alpha^{(t)}|+k}-\\ln(|\\alpha^{(t)}|+k)\\\\\n",
    "&= - \\frac{|\\alpha|}{|\\alpha^{(t)}|+k}+\\frac{|\\alpha^{(t)}|}{|\\alpha^{(t)}|+k}-\\ln(|\\alpha^{(t)}|+k)\\\\\n",
    "&=-\\frac{|\\alpha|}{|\\alpha^{(t)}|+k}+c^{(t)}\n",
    "\\end{align}\n",
    "$$\n",
    "where $c^{(t)}$.\n",
    "\n",
    "* From the above analysis, we get the lower bound for the log likelihood function. Then we have the surrogate function to be as follow:\n",
    "$$\n",
    "g(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$\n",
    "where $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, and  $c^{(t)}$ is a constant irrelevant to optimization.\n",
    "\n",
    "* Then MM updates for $\\alpha_j^{(t+1)}$ can be derived by setting first derivative of the surrogate function to 0:\n",
    "$$\n",
    "\\frac{\\partial g(\\alpha|\\alpha^{(t)})}{\\partial \\alpha_j}=- \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}+\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\frac{1}{\\alpha_j}:=0\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial^2 g(\\alpha|\\alpha^{(t)})}{\\partial \\alpha_j^2}=-\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\frac{1}{\\alpha_j^2}<0\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$\n",
    "\n",
    "\n",
    "* The quantities $s_{jk}$, $r_k$, $\\max_i x_{ij}$ and $\\max_i |\\mathbf{x}_i|$ only depend on data and can be pre-computed and are nonnegative. Therefore, if the starting point $\\alpha_j^{(0)}$ is positive, we will get a nonnegative update for the parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using MM algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **calculate log-pdf of Dirichlet-multinomial distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_logpdf(x::Vector, α::Vector)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at data point `x`.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(x::Vector, α::Vector)\n",
    "    # store the sum of x and α\n",
    "    sumx = sum(x)\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    # store length of x\n",
    "    lengthx = length(x) \n",
    "    \n",
    "    # part without summation\n",
    "    logden = lfact(sumx) - lgamma(sumα + sumx) + lgamma(sumα)\n",
    "    \n",
    "    # part within summation\n",
    "    for j in 1:lengthx\n",
    "        logden += lgamma(α[j] + x[j]) - lgamma(α[j]) - lfact(x[j])  \n",
    "    end\n",
    "    \n",
    "    # return log density\n",
    "    return logden\n",
    "end\n",
    "\n",
    "function dirmult_logpdf!(r::Vector, X::Matrix, α::Vector)\n",
    "    for j in 1:size(X, 2)\n",
    "        r[j] = dirmult_logpdf(X[:, j], α)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at each data point in `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::Matrix, α::Vector)\n",
    "    r = zeros(size(X, 2))\n",
    "    dirmult_logpdf!(r, X, α)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **function to calculate score**\n",
    "\n",
    "$$\n",
    "\\nabla L(\\alpha) \n",
    "= \\sum_{i = 1}^n (\\Psi(\\alpha_j+x_{ij})-\\Psi(\\alpha_j)) - \\sum_{i=1}^n(\\Psi(|\\alpha|+|x_i|)-\\Psi(|\\alpha|))\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_score (generic function with 1 method)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pkg.add(\"SpecialFunctions.jl\")\n",
    "using SpecialFunctions\n",
    "\n",
    "# define the function to calculate score function of log-likelihood\n",
    "# take column of X as one data point\n",
    "function dirmult_score(X::Matrix, α::Vector)\n",
    "    # initialize output \n",
    "    score = zeros(length(α))\n",
    "    \n",
    "    # sum of alpha vector\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    # column sum of X\n",
    "    colsumX = sum(X, 1)\n",
    "    \n",
    "    # store the size of n and d\n",
    "    n = size(X, 2)\n",
    "    d = size(X, 1)\n",
    "    \n",
    "    # evalutate score function at input α\n",
    "    for j in 1:d\n",
    "        for i in 1:n\n",
    "            score[j] += digamma(α[j] + X[j, i]) - digamma(α[j]) - \n",
    "                digamma(sumα + colsumX[i]) + digamma(sumα)\n",
    "            end \n",
    "    end\n",
    "    \n",
    "    # return vector score\n",
    "    return score \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **function to calculate observed info matrix**\n",
    "\n",
    "The observed info matrix can be written as $\\mathbf{D}-m\\mathbf{1}\\mathbf{1}'$, where $\\mathbf{D}$ and $m$ are defined as follow:\n",
    "\n",
    "$j_{th}$ diagonal position in matrix $D$,\n",
    "$$d_j=\\sum_{i = 1}^n[\\Psi'(\\alpha_j)-\\Psi'(\\alpha_j+x_{ij})]$$\n",
    "and \n",
    "$$\n",
    "m= \\sum_{i = 1}^n[\\Psi'(|\\alpha|)-\\Psi'(|\\alpha|+|x_i|)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_obs (generic function with 1 method)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to calculate the observed information matrix \n",
    "# return diagonal vector d and constant m for recovery\n",
    "# take column of X as one data point\n",
    "function dirmult_obs(X::Matrix, α::Vector)\n",
    "    # obs matrix = Diagonal(d) - m11'\n",
    "    # initialization \n",
    "    d = zeros(length(α))\n",
    "    m = 0.0\n",
    "    \n",
    "    # store the size of n and number of parameters\n",
    "    n = size(X, 2)\n",
    "    par = size(X, 1)\n",
    "    \n",
    "    # sum of alpha vector\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    # column sum of X\n",
    "    colsumX = sum(X, 1)\n",
    "    \n",
    "    # evaluation d \n",
    "    for j in 1:par\n",
    "        for i in 1:n\n",
    "            d[j] += trigamma(α[j]) - trigamma(α[j] + X[j, i])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # evaluation m\n",
    "    for i in 1:n\n",
    "        m += trigamma(sumα) - trigamma(sumα + colsumX[i])\n",
    "    end\n",
    "    \n",
    "    return m, d\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **initial $\\mathbf{α}$**\n",
    "\n",
    "The $k^{th}$ element in initial $\\mathbf{α}$ is \n",
    "$$\n",
    "\\alpha_k = \\frac{d-c}{c-1}\\frac{\\sum_i x_{ik}}{\\sum_i|\\mathbf{x_i}|}\n",
    "$$\n",
    "where $c=\\sum_k\\frac{\\sum_{i=1}^n(\\frac{x_{ik}}{|x_i|})^2}{\\sum_{i=1}^n\\frac{x_{ik}}{|x_i|}}$\n",
    "and \n",
    "$d$ is the length of $\\mathbf{α}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_α0 (generic function with 1 method)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to calculate the initial value for α\n",
    "# take column of X as one data point\n",
    "function dirmult_α0(X::Matrix)\n",
    "    # store the size of n and number of parameters \n",
    "    n = size(X, 2)\n",
    "    d = size(X, 1)\n",
    "    \n",
    "    # initialization\n",
    "    α0 = zeros(d)\n",
    "    c = 0.0\n",
    "    \n",
    "    # column sum of X\n",
    "    colsumX = sum(X, 1)\n",
    "    \n",
    "    # total of X\n",
    "    total = sum(colsumX)\n",
    "    \n",
    "    # evaluate c \n",
    "    for k in 1:d\n",
    "        denumerator = sum(X[k, :] ./ colsumX)\n",
    "        if  denumerator > 0\n",
    "            c += sum(X[k, :].^2 ./ colsumX.^2) / denumerator \n",
    "        end\n",
    "    end\n",
    "    \n",
    "        \n",
    "    # evaluate α0 and make sure αsum is nonegative\n",
    "    α0 = max((d - c) / (c - 1), 1e-4) / total * sum(X, 2)\n",
    "    \n",
    "    return α0\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Implement of Newton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_newton"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_newton(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using Newton's method.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `n`-by-`d` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: a `d` vector of starting point (optional). \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "* `maximum`: the log-likelihood at MLE.   \n",
    "* `estimate`: the MLE. \n",
    "* `gradient`: the gradient at MLE. \n",
    "* `hessian`: the Hessian at MLE. \n",
    "* `se`: a `d` vector of standard errors. \n",
    "* `iterations`: the number of iterations performed.\n",
    "\"\"\"\n",
    "\n",
    "function dirmult_newton(X::Matrix; α0::Vector{Float64} = vec(dirmult_α0(X)), \n",
    "            maxiters::Int = 100, tolfun::Float64 = 1e-8)\n",
    "    # store original size of α\n",
    "    α_d = size(X, 1)\n",
    "    \n",
    "    # remove rows with all 0 entries\n",
    "    row_ind = find(sum(X, 2))\n",
    "    \n",
    "    # get the new X removing all the 0 sum lines\n",
    "    X_nz = X[row_ind, :]\n",
    "    \n",
    "    # store the number of parameters \n",
    "    d = size(X_nz, 1)\n",
    "    \n",
    "    # initialization\n",
    "    α_old = α0[row_ind]\n",
    "    \n",
    "    logl_old = sum(dirmult_logpdf(X_nz, α_old))\n",
    "    \n",
    "    # pre-allocation \n",
    "    α_new = zeros(d)\n",
    "    gradient = zeros(d)\n",
    "    \n",
    "    logl_new = 0.0\n",
    "    newtondir = zeros(d)\n",
    "    \n",
    "    # obs matrix = diagonal(D_diag) - m11'\n",
    "    D_diag = zeros(d)\n",
    "    m = 0.0\n",
    "    # check for pd remedy \n",
    "    m_bound = 0.0 \n",
    "    \n",
    "    # iteration count\n",
    "    iteration = 0\n",
    "    \n",
    "    # Newton loop\n",
    "    for iter in 1:maxiters\n",
    "        # evaluate gradient (score)\n",
    "        gradient = dirmult_score(X_nz, α_old)\n",
    "        \n",
    "        # compute inverse obs matrix\n",
    "        m, D_diag = dirmult_obs(X_nz, α_old)\n",
    "        \n",
    "        # remedy for making sure obs matrix is pd\n",
    "        sum_dinv = sum(1 ./ D_diag)\n",
    "        \n",
    "        if m >= 1 / sum_dinv\n",
    "            m = 0.90 * 1 / sum_dinv\n",
    "        end\n",
    "        \n",
    "        # compute Newton's direction based on Sherman-Morrison formula\n",
    "        newtondir .= (1 ./ D_diag) .* gradient + 1 / (1 / m - sum_dinv) *\n",
    "            ((1 ./ D_diag).^2) .* gradient\n",
    "        \n",
    "        # check nonegativity condition for step halving process\n",
    "        step = 1.0\n",
    "            \n",
    "        for i in 1:d\n",
    "            if newtondir[i] < 0\n",
    "               step = min(step, abs(α_old[i] / newtondir[i]) * 0.90)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # line search loop\n",
    "        for lsiter in 1:10\n",
    "            # step halving\n",
    "            step = step / 2^(lsiter - 1)\n",
    "            \n",
    "            # get new α\n",
    "            α_new .= α_old + step .* newtondir\n",
    "            logl_new = sum(dirmult_logpdf(X_nz, α_new))\n",
    "            \n",
    "            # check for getting higher loglikelihood fcn\n",
    "            if logl_new > logl_old\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # check convergence criterion\n",
    "        if abs(logl_new - logl_old) < tolfun * (abs(logl_old) + 1)\n",
    "            # update before break the loop\n",
    "            α_old = copy(α_new)\n",
    "            logl_old = logl_new\n",
    "            iteration += 1\n",
    "            break;\n",
    "        end\n",
    "        \n",
    "        # update \n",
    "        α_old = copy(α_new)\n",
    "        logl_old = logl_new\n",
    "        iteration += 1\n",
    "        \n",
    "    end\n",
    "    \n",
    "    # add zeros back to α vector\n",
    "    α = zeros(α_d)\n",
    "    α[row_ind] = α_old[1:d]\n",
    "    \n",
    "    # compute logl, gradient, Hessian from final iterate\n",
    "    gradient = dirmult_score(X_nz, α_old)\n",
    "    m, D_diag = dirmult_obs(X_nz, α_old)\n",
    "    hessian = (Diagonal(vec(D_diag)) - m * ones(d, d)) * (-1)\n",
    "    \n",
    "    # printout \n",
    "    print(\"In iteratiton $iteration, the log likelihood is $logl_old.\\n\")\n",
    "\n",
    "    # output\n",
    "    return α, logl_old, gradient, hessian, m, D_diag, iteration\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Implement of MM Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_mm"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_mm(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using MM algorithm.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `d`-by-`n` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: starting point. \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "# Output\n",
    "* `logl`: the log-likelihood at MLE.   \n",
    "* `niter`: the number of iterations performed.\n",
    "# `α`: the MLE.\n",
    "* `∇`: the gradient at MLE. \n",
    "* `obsinfo`: the observed information matrix at MLE. \n",
    "\"\"\"\n",
    "function dirmult_mm(\n",
    "    X::AbstractMatrix; \n",
    "    α0::Vector = vec(dirmult_α0(X)), \n",
    "    maxiters::Int = 100, \n",
    "    tolfun = 1e-6\n",
    "    )\n",
    "    \n",
    "    # store original size of α\n",
    "    α_d = size(X, 1)\n",
    "    \n",
    "    # remove rows with all 0 entries\n",
    "    row_ind = find(sum(X, 2))\n",
    "    \n",
    "    # get the new X removing all the 0 sum lines\n",
    "    X_nz = X[row_ind, :]\n",
    "    \n",
    "    # store the number of parameters \n",
    "    d = size(X_nz, 1)\n",
    "    \n",
    "    # initialization\n",
    "    α_old = α0[row_ind]\n",
    "    logl_old = sum(dirmult_logpdf(X_nz, α_old))\n",
    "    \n",
    "    # find max for certain j or say for certain row \n",
    "    max_Xij = maximum(X_nz, 2)\n",
    "    \n",
    "    # find largest col sum as max of Xi\n",
    "    colsum = sum(X_nz, 1)\n",
    "    max_Xi = maximum(colsum)\n",
    "\n",
    "    # pre-allocation\n",
    "    α_new = zeros(d)\n",
    "    logl_new = 0.0\n",
    "    \n",
    "    # iteration count\n",
    "    iteration = 0\n",
    "    \n",
    "    for iter in 1:maxiters\n",
    "        ## MM update   \n",
    "        \n",
    "        # calculate the denominator \n",
    "        denominator = 0.0\n",
    "        for k in 0:(max_Xi - 1)\n",
    "            # calculate r_k\n",
    "            r_k = count(colsum .> k)\n",
    "            \n",
    "            denominator += r_k / (sum(α_old) + k)\n",
    "        end\n",
    "        \n",
    "        # calculate the numerator \n",
    "        numerator = 0.0\n",
    "        for j in 1:d\n",
    "            s_jk = 0.0\n",
    "            \n",
    "            # max Xij - 1 cannot be less than 0\n",
    "            if  max_Xij[j] - 1 >= 0\n",
    "                for k in 0:(max_Xij[j] - 1)\n",
    "                    # calculate s_jk\n",
    "                    s_jk = sum(X_nz[j, :] .> k)\n",
    "                    numerator += s_jk / (α_old[j] + k)\n",
    "                end\n",
    "            end\n",
    "            # store α_new\n",
    "            α_new[j] = α_old[j] * numerator / denominator \n",
    "        end\n",
    "        \n",
    "        # calculate logpdf with new α\n",
    "        logl_new = sum(dirmult_logpdf(X_nz, α_new))\n",
    "        \n",
    "        # check convergence criterion\n",
    "        if abs.(logl_new - logl_old) < tolfun * (abs.(logl_old) + 1)\n",
    "            # update before break the loop\n",
    "            α_old = copy(α_new)\n",
    "            logl_old = logl_new\n",
    "            iteration += 1\n",
    "            break;\n",
    "        end\n",
    "        \n",
    "        # update \n",
    "        α_old = copy(α_new)\n",
    "        logl_old = logl_new\n",
    "        iteration += 1\n",
    "    end\n",
    "    \n",
    "    # add zeros back to α vector\n",
    "    α = zeros(α_d)\n",
    "    α[row_ind] = α_old[1:d]\n",
    "    \n",
    "    # compute logl, gradient, Hessian from final iterate\n",
    "    gradient = dirmult_score(X_nz, α_old)\n",
    "    m, D_diag = dirmult_obs(X_nz, α_old)\n",
    "    hessian = (Diagonal(vec(D_diag)) - m * ones(d, d)) * (-1)\n",
    "    \n",
    "    # printout \n",
    "    print(\"In iteratiton $iteration, the log likelihood is $logl_old.\\n\")\n",
    "\n",
    "    # output\n",
    "    return α, logl_old, gradient, hessian, m, D_diag, iteration\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Re-do [HW4 Q9](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html#Q9) using your new `dirmult_mm` function. Compare the number of iterations and run time by MM algorithm to those by Newton's method. Comment on the efficiency of Newton's algorithm vs MM algorithm for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in train data\n",
    "path1_tra = \"http://hua-zhou.github.io/teaching/biostatm280\"\n",
    "path2_tra = \"-2018spring/hw/hw4/optdigits.tra\"\n",
    "\n",
    "train = readdlm(download(path1_tra * path2_tra), ','); \n",
    "\n",
    "#Pkg.add(\"DataFrames\")\n",
    "#Pkg.add(\"Distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton Method results: \n",
      "In iteratiton 100, the log likelihood is -37358.4447603111.\n",
      "MM Algorithm results: \n",
      "logl_old = -37986.45956075273\n",
      "size(max_Xij) = (48, 1)\n",
      "size(α_new) = (48,)\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "In iteratiton 10, the log likelihood is -190654.82023861312.\n",
      "Newton Method results: \n",
      "In iteratiton 35, the log likelihood is -42179.2463942298.\n",
      "MM Algorithm results: \n",
      "logl_old = -42492.22854512651\n",
      "size(max_Xij) = (52, 1)\n",
      "size(α_new) = (52,)\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "In iteratiton 10, the log likelihood is -217757.74438351995.\n",
      "Newton Method results: \n",
      "In iteratiton 36, the log likelihood is -39985.26533251815.\n",
      "MM Algorithm results: \n",
      "logl_old = -40253.48124197674\n",
      "size(max_Xij) = (52, 1)\n",
      "size(α_new) = (52,)\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "In iteratiton 9, the log likelihood is -190987.349671325.\n",
      "Newton Method results: \n",
      "In iteratiton 52, the log likelihood is -40519.472606903226.\n",
      "MM Algorithm results: \n",
      "logl_old = -40767.383570172766\n",
      "size(max_Xij) = (53, 1)\n",
      "size(α_new) = (53,)\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "In iteratiton 10, the log likelihood is -192722.4045599366.\n",
      "Newton Method results: \n",
      "In iteratiton 20, the log likelihood is -43488.774019869525.\n",
      "MM Algorithm results: \n",
      "logl_old = -43800.317989338066\n",
      "size(max_Xij) = (58, 1)\n",
      "size(α_new) = (58,)\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "In iteratiton 10, the log likelihood is -195831.45593965473.\n",
      "Newton Method results: \n",
      "In iteratiton 30, the log likelihood is -41191.31029823919.\n",
      "MM Algorithm results: \n",
      "logl_old = -41493.30254265043\n",
      "size(max_Xij) = (55, 1)\n",
      "size(α_new) = (55,)\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "In iteratiton 10, the log likelihood is -184473.45908170185.\n",
      "Newton Method results: \n",
      "In iteratiton 50, the log likelihood is -37702.51162948245.\n",
      "MM Algorithm results: \n",
      "logl_old = -37845.30662203098\n",
      "size(max_Xij) = (49, 1)\n",
      "size(α_new) = (49,)\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "In iteratiton 10, the log likelihood is -190515.58348464628.\n",
      "Newton Method results: \n",
      "In iteratiton 30, the log likelihood is -40303.99812799373.\n",
      "MM Algorithm results: \n",
      "logl_old = -40553.93367470497\n",
      "size(max_Xij) = (51, 1)\n",
      "size(α_new) = (51,)\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "In iteratiton 10, the log likelihood is -196193.6540577286.\n",
      "Newton Method results: \n",
      "In iteratiton 38, the log likelihood is -43130.848850681985.\n",
      "MM Algorithm results: \n",
      "logl_old = -43291.15897116983\n",
      "size(max_Xij) = (51, 1)\n",
      "size(α_new) = (51,)\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "In iteratiton 10, the log likelihood is -209714.56658813864.\n",
      "Newton Method results: \n",
      "In iteratiton 30, the log likelihood is -43709.654694763056.\n",
      "MM Algorithm results: \n",
      "logl_old = -44021.37656674774\n",
      "size(max_Xij) = (54, 1)\n",
      "size(α_new) = (54,)\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "size(logl_new) = ()\n",
      "size(logl_old) = ()\n",
      "In iteratiton 10, the log likelihood is -197236.62396241564.\n"
     ]
    }
   ],
   "source": [
    "using DataFrames, Distributions\n",
    "\n",
    "# convert the training data set to Int64\n",
    "X = round.(Int64, train);\n",
    "\n",
    "# initialize matrix to store α, iteration, logl and runtime\n",
    "α_newton = zeros(64, 10)\n",
    "α_mm = zeros(64, 10)\n",
    "\n",
    "logl_newton = zeros(10)\n",
    "logl_mm = zeros(10)\n",
    "\n",
    "iteration_newton = zeros(10)\n",
    "iteration_mm = zeros(10)\n",
    "\n",
    "runtime_newton = zeros(10)\n",
    "runtime_mm = zeros(10)\n",
    "\n",
    "for digit in 0:9\n",
    "    # get the train_x subset for different digits\n",
    "    X_digit = (X[X[:, 65] .== digit, 1:64])'\n",
    "    \n",
    "    # store the MLE for α\n",
    "    print(\"Newton Method results: \\n\")\n",
    "    α_newton[:, (digit + 1)] .= dirmult_newton(X_digit)[1]\n",
    "    \n",
    "    print(\"MM Algorithm results: \\n\")\n",
    "    α_mm[:, (digit + 1)] .= dirmult_mm(X_digit)[1]\n",
    "\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Int64,1}:\n",
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit = collect(0:9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
