{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat280 HW3\n",
    "* Author: Shuang Gao\n",
    "* Date: 2018/05/23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 - Big $n$ regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(1)\n",
    "\n",
    "Download the flight data from <http://stat-computing.org/dataexpo/2009/the-data.html>. For this exercise, we only need data from years 2003-2008. If you are using Mac or Linux, you can run the following Bash script, which downloads and unzips files for all years.\n",
    "```bash\n",
    "# Download flight data by year\n",
    "for i in {1987..2008}\n",
    "  do\n",
    "    echo \"$(date) $i Download\"\n",
    "    fnam=$i.csv.bz2\n",
    "    wget -O ./$fnam http://stat-computing.org/dataexpo/2009/$fnam\n",
    "    echo \"$(date) $i unzip\"\n",
    "    bzip2 -d ./$fnam\n",
    "  done\n",
    "\n",
    "# Download airline carrier data\n",
    "wget -O ./airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS\n",
    "\n",
    "# Download airports data\n",
    "wget -O ./airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\n",
    "```\n",
    "Find out how many data points in each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In year 2003, there are 6488541 data points. \n",
      "In year 2004, there are 7129271 data points. \n",
      "In year 2005, there are 7140597 data points. \n",
      "In year 2006, there are 7141923 data points. \n",
      "In year 2007, there are 7453216 data points. \n",
      "In year 2008, there are 7009729 data points. \n",
      "There are 42363277 data points in total."
     ]
    }
   ],
   "source": [
    "# change working directory to flight folder \n",
    "cd(\"D:\\\\UCLA\\\\quarter3\\\\BS280\\\\biostat-m280-2018-spring\\\\hw3\\\\flight\")\n",
    "\n",
    "# calculate total data \n",
    "count = 0\n",
    "\n",
    "# print number of data points in each year\n",
    "for i in 2003:2008\n",
    "    print(\" In year $i, there are \", countlines(\"$i\" * \".csv\"), \n",
    "        \" data points. \\n\")\n",
    "    count = count + countlines(\"$i\" * \".csv\")\n",
    "end\n",
    "\n",
    "print(\" There are $count data points in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(2) \n",
    "\n",
    "We are interested in how the time gain of a flight, defined as `DepDelay - ArrDelay`, depends on the distance traveled (`Distance`), departure delay (`DepDelay`), and carrier (`UniqueCarrier`). \n",
    "\n",
    "We want to fit a linear regression `Gain ~ 1 + Distance + DepDelay + UniqueCarrier` using data from 2003-2008. Note `UniqueCarrier` is a factor with 23 levels: \"9E\", \"AA\", \"AQ\", \"AS\", \"B6\", \"CO\", \"DH\", \"DL\", \"EV\", \"F9\", \"FL\", \"HA\", \"HP\", \"MQ\", \"NW\", \"OH\", \"OO\", \"TZ\", \"UA\", \"US\", \"WN\", \"XE\", and \"YV\". We use the dummy coding with \"9E\" as base level.\n",
    "\n",
    "Will the design matrix (in double precision) fit into the memory of you computer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Solution:**\n",
    "* There are data points in total. For each row in design matrix, there are 26 predictors. In double precision, the whole design matrix will take up $42363277*26*8/10^9 \\approx 8.81$ GB. This data set can not fit into the memory of my computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(3)\n",
    "\n",
    "Review the [Summary of Linear Regression](http://hua-zhou.github.io/teaching/biostatm280-2018spring/slides/12-linreg/linreg.html) and devise a strategy to solve the linear regression.\n",
    "\n",
    "Report the estimated regression coefficients $\\widehat \\beta$, estimated variance $\\widehat \\sigma^2 = \\sum_i (y_i - \\widehat y_i)^2 / (n - 1)$, and coefficient standard errors.\n",
    "\n",
    "Hint: It took my laptop less than 3 minutes to import data and fit linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant var2col\n",
      "WARNING: redefining constant col2var\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "generate_xy (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping from variable names to X columns\n",
    "# carrier \"9E\" is used as base level\n",
    "const var2col = Dict(\n",
    "        \"Intercept\" => 1,\n",
    "        \"Distance\" => 2,\n",
    "        \"DepDelay\" => 3,\n",
    "        \"AA\" => 4,\n",
    "        \"AQ\" => 5,\n",
    "        \"AS\" => 6,\n",
    "        \"B6\" => 7,\n",
    "        \"CO\" => 8,\n",
    "        \"DH\" => 9,\n",
    "        \"DL\" => 10,\n",
    "        \"EV\" => 11,\n",
    "        \"F9\" => 12,\n",
    "        \"FL\" => 13,\n",
    "        \"HA\" => 14,\n",
    "        \"HP\" => 15,\n",
    "        \"MQ\" => 16,\n",
    "        \"NW\" => 17,\n",
    "        \"OH\" => 18,\n",
    "        \"OO\" => 19,\n",
    "        \"TZ\" => 20,\n",
    "        \"UA\" => 21,\n",
    "        \"US\" => 22,\n",
    "        \"WN\" => 23,\n",
    "        \"XE\" => 24,\n",
    "        \"YV\" => 25,\n",
    "        \"Gain\" => 26)\n",
    "# mapping from column to variable names\n",
    "const col2var = map(reverse, var2col)\n",
    "\n",
    "# a custom function to generate [X y] from data table\n",
    "function generate_xy(tbl::NextTable)\n",
    "    # X matrix\n",
    "    XY = zeros(length(tbl), 26)\n",
    "    # intercept term\n",
    "    @views fill!(XY[:, 1], 1)\n",
    "    # Distance term\n",
    "    @views copy!(XY[:, 2], columns(tbl, :Distance))\n",
    "    # DepDelay term\n",
    "    @views copy!(XY[:, 3], columns(tbl, :DepDelay))\n",
    "    # Dummy coding for airline\n",
    "    @inbounds for i in 1:length(tbl)\n",
    "        tbl[i][:UniqueCarrier] == \"9E\" && continue # base level\n",
    "        XY[i, var2col[tbl[i][:UniqueCarrier]]] = 1\n",
    "    end\n",
    "    # last column is response: gain = depdelay - arrdelay\n",
    "    XY[:, 26] = select(tbl, \n",
    "        (:DepDelay, :ArrDelay) => p -> Float64(p.DepDelay - p.ArrDelay))\n",
    "    # return\n",
    "    XY\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from csv\n",
    "using JuliaDB\n",
    "\n",
    "# only need columns: DepDelay, ArrDelay, UniqueCarrier, Distance\n",
    "table1 = loadtable(\n",
    "    \"2003.csv\", \n",
    "    datacols = [\"DepDelay\", \"ArrDelay\", \"UniqueCarrier\", \"Distance\"])\n",
    "\n",
    "# drop rows with missing values\n",
    "table1 = dropna(table1);\n",
    "\n",
    "# only need columns: DepDelay, ArrDelay, UniqueCarrier, Distance\n",
    "table2 = loadtable(\n",
    "    \"2004.csv\", \n",
    "    datacols = [\"DepDelay\", \"ArrDelay\", \"UniqueCarrier\", \"Distance\"])\n",
    "\n",
    "# drop rows with missing values\n",
    "table2 = dropna(table2);\n",
    "\n",
    "# only need columns: DepDelay, ArrDelay, UniqueCarrier, Distance\n",
    "table3 = loadtable(\n",
    "    \"2005.csv\", \n",
    "    datacols = [\"DepDelay\", \"ArrDelay\", \"UniqueCarrier\", \"Distance\"])\n",
    "\n",
    "# drop rows with missing values\n",
    "table3 = dropna(table3);\n",
    "\n",
    "# only need columns: DepDelay, ArrDelay, UniqueCarrier, Distance\n",
    "table4 = loadtable(\n",
    "    \"2006.csv\", \n",
    "    datacols = [\"DepDelay\", \"ArrDelay\", \"UniqueCarrier\", \"Distance\"])\n",
    "\n",
    "# drop rows with missing values\n",
    "table4 = dropna(table4);\n",
    "\n",
    "# only need columns: DepDelay, ArrDelay, UniqueCarrier, Distance\n",
    "table5 = loadtable(\n",
    "    \"2007.csv\", \n",
    "    datacols = [\"DepDelay\", \"ArrDelay\", \"UniqueCarrier\", \"Distance\"])\n",
    "\n",
    "# drop rows with missing values\n",
    "table5 = dropna(table5);\n",
    "\n",
    "# only need columns: DepDelay, ArrDelay, UniqueCarrier, Distance\n",
    "table6 = loadtable(\n",
    "    \"2008.csv\", \n",
    "    datacols = [\"DepDelay\", \"ArrDelay\", \"UniqueCarrier\", \"Distance\"])\n",
    "\n",
    "# drop rows with missing values\n",
    "table6 = dropna(table6);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pkg.add(\"SweepOperator\")\n",
    "using SweepOperator\n",
    "\n",
    "# generate XY for each year\n",
    "xy1 = generate_xy(table1)\n",
    "xy2 = generate_xy(table2)\n",
    "xy3 = generate_xy(table3)\n",
    "xy4 = generate_xy(table4)\n",
    "xy5 = generate_xy(table5)\n",
    "xy6 = generate_xy(table6)\n",
    "\n",
    "# form the augmented Gram matrix for each year\n",
    "G1 = xy1' * xy1\n",
    "G2 = xy2' * xy2\n",
    "G3 = xy3' * xy3\n",
    "G4 = xy4' * xy4\n",
    "G5 = xy5' * xy5\n",
    "G6 = xy6' * xy6\n",
    "\n",
    "\n",
    "# use sweep function to do linear regression\n",
    "G = G1 + G2 + G3 + G4 + G5 + G6\n",
    "sweep!(G, 1:25);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The estimated regression coefficients are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25-element Array{Float64,1}:\n",
       "  1.14033   \n",
       "  0.00164935\n",
       " -0.0118811 \n",
       " -1.8723    \n",
       " -0.5789    \n",
       " -0.938452  \n",
       " -1.42247   \n",
       " -2.57627   \n",
       "  1.16808   \n",
       " -2.19625   \n",
       "  1.03932   \n",
       " -2.15207   \n",
       " -1.35247   \n",
       " -1.87248   \n",
       " -0.350758  \n",
       " -1.46395   \n",
       " -3.62506   \n",
       " -0.00722279\n",
       " -0.40365   \n",
       " -3.5774    \n",
       " -1.14816   \n",
       " -0.883753  \n",
       "  2.74855   \n",
       " -2.56721   \n",
       " -0.202211  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output regression coefficeints\n",
    "G[1:25, 26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204.40310249917593"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate estimated variance\n",
    "num = size(xy1, 1) + size(xy2, 1) + size(xy3, 1) + size(xy4, 1) +\n",
    "    size(xy5, 1) + size(xy6, 1)\n",
    "σ_sq = G[26, 26] / num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The estimated variance $\\widehat \\sigma^2 = \\sum_i (y_i - \\widehat y_i)^2 / (n - 1)$ is around 204."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* coefficient standard errors are shown in the vector below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25-element Array{Float64,1}:\n",
       " 0.0202318 \n",
       " 4.37958e-6\n",
       " 6.88878e-5\n",
       " 0.0215571 \n",
       " 0.0521477 \n",
       " 0.0250361 \n",
       " 0.0259154 \n",
       " 0.0229534 \n",
       " 0.0266552 \n",
       " 0.021659  \n",
       " 0.023012  \n",
       " 0.0319265 \n",
       " 0.023868  \n",
       " 0.0339852 \n",
       " 0.0276918 \n",
       " 0.0218055 \n",
       " 0.0220088 \n",
       " 0.023446  \n",
       " 0.0217463 \n",
       " 0.0375073 \n",
       " 0.0219444 \n",
       " 0.0220022 \n",
       " 0.0209201 \n",
       " 0.0222383 \n",
       " 0.0255715 "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(diag(-G[1:25, 1:25]) * σ_sq).^(0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 - Google PageRank\n",
    "\n",
    "We are going to try different numerical methods learnt in class on the [Google PageRank problem](https://en.wikipedia.org/wiki/PageRank)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q2(1)\n",
    "\n",
    "Let $\\mathbf{A} \\in \\{0,1\\}^{n \\times n}$ be the connectivity matrix of $n$ web pages with entries\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\ta_{ij}= \\begin{cases}\n",
    "\t1 & \\text{if page $i$ links to page $j$} \\\\\n",
    "\t0 & \\text{otherwise}\n",
    "\t\\end{cases}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "$r_i = \\sum_j a_{ij}$ is the out-degree of page $i$. That is $r_i$ is the number of links on page $i$. Imagine a random surfer exploring the space of $n$ pages according to the following rules.  \n",
    "\n",
    "- From a page $i$ with $r_i>0$\n",
    "    * with probability $p$, (s)he randomly chooses a link on page $i$ (uniformly) and follows that link to the next page  \n",
    "    * with probability $1-p$, (s)he randomly chooses one page from the set of all $n$ pages (uniformly) and proceeds to that page  \n",
    "- From a page $i$ with $r_i=0$ (a dangling page), (s)he randomly chooses one page from the set of all $n$ pages (uniformly) and proceeds to that page  \n",
    "    \n",
    "The process defines a Markov chain on the space of $n$ pages. Write the transition matrix $\\mathbf{P}$ of the Markov chain as a sparse matrix plus rank 1 matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "* Each element $p_{ij}$ in the transition matrix $P$ represents the probability of transiting from page $i$ to page $j$. There are two posibility for $p_{ij}$: \n",
    "    0. If page $i$ has no connection to other pages ($r_i=0$), then transiting to page $j$ will only happen when the next random choice of the $n$ pages to be $j$.\n",
    "    0. If page $i$ has connection to other pages ($r_i>0$), visitor can transit to page $j$ with the $ij$ connection with probability $p$ or by randomly chosing a next page from all $n$ pages with probability $1-p$.\n",
    "    \n",
    "* The transition matrix is shown below:\n",
    "$$\n",
    "\\mathbf{P}_{ij}=\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "    \\frac{1-p}{n} + \\frac{pa_{ij}}{r_i} \\text{ for } r_i>0 \\\\\n",
    "    \\frac{1}{n} \\text{ for } r_i=0\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "* Matrix $\\mathbf{P}$ can be further represented by two parts:\n",
    "$\\mathbf{P} = \\mathbf{R}\\mathbf{A} + \\mathbf{e}\\mathbf{v}^T$ where $\\mathbf{v}$ is a column vector with n ones,\n",
    "$$\n",
    "\\mathbf{R}=\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "    \\frac{p}{r_i} \\text{ on the diagonal for } r_i>0 \\\\\n",
    "    0 \\text{ otherwise. } \n",
    "    \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "\n",
    "and $\\mathbf{e}$ is a column vector with $n$ elements,\n",
    "$$\n",
    "\\mathbf{e}=\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "    \\frac{1-p}{n} \\text{ for } r_i>0 \\\\\n",
    "    \\frac{1}{n} \\text{ for } r_i=0\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "* Therefore, matrix $\\mathbf{P}$ is composed by a rank 1 matrix, $v$, and a sparse matrix $\\mathbf{A}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(2)\n",
    "\n",
    " According to standard Markov chain theory, the (random) position of the surfer converges to the stationary distribution $\\mathbf{x} = (x_1,\\ldots,x_n)^T$ of the Markov chain. $x_i$ has the natural interpretation of the proportion of times the surfer visits page $i$ in the long run. Therefore $\\mathbf{x}$ serves as page ranks: a higher $x_i$ means page $i$ is more visited. It is well-known that $\\mathbf{x}$ is the left eigenvector corresponding to the top eigenvalue 1 of the transition matrix $\\mathbf{P}$. That is $\\mathbf{P}^T \\mathbf{x} = \\mathbf{x}$. Therefore $\\mathbf{x}$ can be solved as an eigen-problem. Show that it can also be cast as solving a linear system. Since the row sums of $\\mathbf{P}$ are 1, $\\mathbf{P}$ is rank deficient. We can replace the first equation by the $\\sum_{i=1}^n x_i = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Solution: **\n",
    "$$\n",
    "\\because \\mathbf{P}^T \\mathbf{x} = \\mathbf{x}\\\\\n",
    "\\therefore (\\mathbf{P}^T-\\mathbf{I}) \\mathbf{x} = \\mathbf{0}\n",
    "$$\n",
    "* From the above equation, it is shown that the problem can be represented by linear equation. However, because row $i$ in $\\mathbf{P}$ represents the probability of transiting from page $i$ to all other pages, the row sum of $\\mathbf{P}$ is 1 for all rows. Thus, the row sum of $\\mathbf{P}^T-\\mathbf{I}$ is 0 for all rows. Therefore, one column of the matrix $\\mathbf{P}^T-\\mathbf{I}$ can add with all other columns to get an all $0$s column. This results in rank deficiency. \n",
    "* Therefore, we can replace the first equation by the $\\sum_{i=1}^n x_i = 1$. That is, replacing all the elements in first row of $\\mathbf{P}^T-\\mathbf{I}$ by $1$ and also replace the first element of right hand side vector by $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(3)\n",
    "\n",
    "Download the [`ucla.zip`](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw3/ucla.zip) package from course webpage. Unzip the package, which contains two files `U.txt` and `A.txt`. `U.txt` lists the 500 URL names. `A.txt` is the $500 \\times 500$ connectivity matrix. Read data into Julia. Compute summary statistics:\n",
    "* number of pages\n",
    "* number of edges\n",
    "* number of dangling nodes (pages with no out links)\n",
    "* which page has max in-degree?\n",
    "* which page has max out-degree?\n",
    "* visualize the sparsity pattern of $\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working directory to UCLA folder \n",
    "cd(\"D:\\\\UCLA\\\\quarter3\\\\BS280\\\\biostat-m280-2018-spring\\\\hw3\\\\ucla\")\n",
    "\n",
    "# readin data A and U\n",
    "A = readdlm(\"A.txt\", ',')\n",
    "U = readdlm(\"U.txt\", ',');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* number of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pages in the study is 500."
     ]
    }
   ],
   "source": [
    "print(\"The number of pages in the study is \", size(U, 1), \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* number of edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of edges in the study is 11158."
     ]
    }
   ],
   "source": [
    "print(\"The number of edges in the study is \", nnz(sparse(A)), \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* number of dangling nodes(pages with no out links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of dangling nodes is 96."
     ]
    }
   ],
   "source": [
    "rowsum = sum(sparse(A), 2);\n",
    "\n",
    "print(\"The number of dangling nodes is \", \n",
    "    500 - size(rowsum[rowsum .> 0], 1), \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which page has max in-degree?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page http://www.ucla.edu has max in-degree 171.0."
     ]
    }
   ],
   "source": [
    "colsum = sum(sparse(A), 1)\n",
    "\n",
    "print(\"The page \", U[findmax(colsum)[2]], \" has max in-degree \", \n",
    "    findmax(colsum)[1], \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which page has max out-degree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page http://giveto.ucla.edu has max in-degree 88.0."
     ]
    }
   ],
   "source": [
    "print(\"The page \", U[findmax(rowsum)[2]], \" has max in-degree \", \n",
    "    findmax(rowsum)[1], \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visualize the sparsity pattern of A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 600 400\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip1200\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"600\" height=\"400\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip1200)\" points=\"\n",
       "0,400 600,400 600,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip1201\">\n",
       "    <rect x=\"120\" y=\"0\" width=\"421\" height=\"400\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip1200)\" points=\"\n",
       "126.039,375.869 490.096,375.869 490.096,11.811 126.039,11.811 \n",
       "  \" fill=\"#ffffff\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip1202\">\n",
       "    <rect x=\"126\" y=\"11\" width=\"365\" height=\"365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  126.039,375.869 490.096,375.869 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  126.039,11.811 126.039,375.869 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  198.486,375.869 198.486,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  271.298,375.869 271.298,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  344.109,375.869 344.109,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  416.921,375.869 416.921,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  489.732,375.869 489.732,370.408 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  126.039,84.2585 131.499,84.2585 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  126.039,157.07 131.499,157.07 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  126.039,229.882 131.499,229.882 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  126.039,302.693 131.499,302.693 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1200)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  126.039,375.505 131.499,375.505 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 198.486, 389.669)\" x=\"198.486\" y=\"389.669\">100</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 271.298, 389.669)\" x=\"271.298\" y=\"389.669\">200</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 344.109, 389.669)\" x=\"344.109\" y=\"389.669\">300</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 416.921, 389.669)\" x=\"416.921\" y=\"389.669\">400</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:middle;\" transform=\"rotate(0, 489.732, 389.669)\" x=\"489.732\" y=\"389.669\">500</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 120.039, 88.7585)\" x=\"120.039\" y=\"88.7585\">100</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 120.039, 161.57)\" x=\"120.039\" y=\"161.57\">200</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 120.039, 234.382)\" x=\"120.039\" y=\"234.382\">300</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 120.039, 307.193)\" x=\"120.039\" y=\"307.193\">400</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1200)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:12; text-anchor:end;\" transform=\"rotate(0, 120.039, 380.005)\" x=\"120.039\" y=\"380.005\">500</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip1202)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  126.039,11.811 490.096,11.811 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip1202)\" style=\"stroke:#000000; stroke-width:1; stroke-opacity:1; fill:none\" points=\"\n",
       "  490.096,11.811 490.096,375.869 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip1202)\">\n",
       "<image width=\"364\" height=\"364\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAWwAAAFsCAYAAADon4O5AAAN/UlEQVR4nO3dP48kRxkH4JrjcGbJ\n",
       "EQIJySRIjlYOCBwiJPgMziDc0KFDdL7zYSKHJEgXQYYcEl5EQuIIEVrwBfgGQ3A3vt7e7pnq//VW\n",
       "PY+02tndmZ0/Xf2bd96u7k5pwE//84s/f/yrf/08pZQ++Of/3n/17P7u8rd/dy7/+It3l1NK6XXv\n",
       "5yEfd67zWcb1r3ny4KfTwO/yffT8/u6T528ez+++evP9lFL67vm7yw/v6aHPnt/fff32un959u42\n",
       "r549vv2QJ4/u45SevXf9Pr/7yV8//P7y885yeXF/99GLNz9/+uL+Ln057XX+7MX93ddvb//Ni/u7\n",
       "7nL9zR9uP5/Lczmlx8/p8vcf3noQncf8ydvLn3x5f/f5xOeSUkpfv73Nqy/v7759e3louY75wcjv\n",
       "T+mU0nvvxtx7A9d59dv/vn+5/LOX7x77py/v7+5fPn5dc33z8v7udef2//gi//mklNLTkd///u2T\n",
       "eJKGn/dnnTH3y85j/rxz+e+955LzmL7t3OaDr6Yv4+54enR/P7qeC6//9usPx/72pxmP5YE/vrv9\n",
       "64nL6GLq9QF46Lzw73Pu6zz2RgvAbed0uwheI7zPKc3vIAAAQPHOve9b3s/W9wFQtaUhmnP7R28K\n",
       "WiIA8y0J7mu3HeyNC2yA+ZbMuhu77WiQ1zTFL2drLcAaLnkzljvd3+tBAxxoi42OY/9LDxtgBWt9\n",
       "qs8KfjvOABwru52rwgaYb43pfdlVusAGWF/uPOtJLRWBDbC+nOOLXLvOYOALbID55mx0zKm+TVEG\n",
       "WMncaX3nkctZ11NhA+xj8c59Ahtge/bEBjjIlFbIHkf2A2BE7inChC3AwXICWVgDFGDPk/B+z0ZH\n",
       "gPmGgnmzytrBnwDWs+lskJoqbP0i4EibT92rKbABjrLLPGuBDbCMnWIACnbIPOuaKmw9bGBPMgcA\n",
       "KIOqBGZ6kqxAANAMb/qwky1WNiswABTIGzRwlZAAmnJK6++lcwlSe/60Z+hN1DiAQLaqhJ12B2jO\n",
       "1sElGNmaMQYAlGWN6uQ88LX1fUJKj8eSsQUAHOvoLfj9GSpTq6OjHz/TOXYwzFTi4VWtzHWzfMl1\n",
       "7l3eaq/sMG20EgMbYIq5gXv5tBemiBDYQHS5gduv2MME9YXABlpxCeiQYZ3SsQ96aBd2Gx2BqaYE\n",
       "cNiwTunYCjvsi8YiYTbwsIktlv+ULAmdOyW2REK/oNxk+bZtyvJfc5ZIqNkgY7REgNqFboN0ldgS\n",
       "qeKFBRZbo8KuJqxTOv6J2NOxPVWtQLCnIyvs8P0kZhHW7KGKnnXf0wPv+3K2m9zfA1zTzY0qC4MS\n",
       "Z4kAMEBgAwQhsIEaNNFGPbKHPaaJF75hZomwhSbGlFki7K2JFYvdVDkbZIyWCFCya2Ec7njWS5W4\n",
       "pyN1a6YaYhXXcqK5DNESYW/NrWSsrqk2SFeJGx0BxjS90brElkizCwN4YOjgT03ng42OAACwpqM/\n",
       "Xji8anua/1gLADXozgBpcibINSWcImwJlRoAAAAQ3NQWif4W1KPZvRenMA8bIAi7pgNHM9VzgqUf\n",
       "Q/q7j479/db1ltz/ta+17w/gEFoiAEEIbOAIPvXOILCBvelZz3T0izZ0LJFTyn/3PfrxA9vp5sMl\n",
       "E5as894oADLlFGLmYwNQhzXezcam0215n5DS47FkbFGtS794j75OCf2jUlfmfp8u57oQQQnrfTXs\n",
       "6QhcU2qR0yTT+gCCENgAQQhsgCD0sMsyZach2MOtDYb9nVtsYNyQwC6LsKY0uTu7XLu+EF+JwC6L\n",
       "CpvSDB06goPoYQO5hsJ6q+PcM0BgA3P1K27V98YENjCH9sgBBDYwlbA+iMAGphDWBxLYQC5hfTCB\n",
       "DeQQ1gUQ2MAtQ6fuM50PAAAAAAAAAAAAAIDiRd5zaemeV/bcYm977GCSO6Yvj+XWSTOsI4VpdS+l\n",
       "Vp83XFgH5jvktYv87qnCJprSKuzuyXOX/j924FgiUD/H/SA8A5cWGfeBRT5rupYG0RzVEuluYLx1\n",
       "It2c/weTqRSApkSusJdSobO3EirsISrsIFoObGiBwqQiZolAnW5V1QQksAGCENhQH22QSglsqE9O\n",
       "WJ+TmVbh2OgIbVKBB6TCBk7p4U41wrxQKmygS2gXTGDDdqJs/LPjTBBaIrCd0sPORsdgBDa0q/Q3\n",
       "FHoENkAQAhsgCIENmNYXhFkiQH/jo42RhVJhQ/1uBbAKOwgVNtSnP//7VgCrsINQYUN9HPypUgKb\n",
       "PQmI4906IS8FE9jsSUAcr7sMvIEGI7ChXd5AgxHYAEEIbIAg9gpsvTKAhfYKbL0ygIW0RAAAAAAA\n",
       "AIAi7TGNz1RB9jR0RDtjEKAygh0AAAAAIAb9XACK4A2pDZYzAABwIB9JgKY4TjU8NKcQyFmPzpnX\n",
       "g93ZNZ3a2DWdahnItMA4BwAAADJFbiNEfuxwKCsPe1o63oxXimVwUhPjGQAAAAAAAIA6nHvfb113\n",
       "7JgK/a+c+7z1O4aVdhyLuWNnC8YWVVtzMC8J7Dn3BWsxnrbjtV3JKa172MfLgnEYSY40JSAu60D3\n",
       "Z2hC7oriHRcAgDppiVCjuS0R45aiPTn6AUBBtOoomsAGCEJgAwQhsKndWF/6NPA3PWyKJrCp3Vhf\n",
       "urS9ReEmgQ0QxF6BPeV4JdRJRQsLPd3pfk6977THsoeFtEQojSocAIjt8jF1i6rGR2CAjSwJ7akn\n",
       "MKA955HLkdXyPAhCD5u9nEYuR1bL8yCQqFVC1McNUAQhCgAAW1TFc/6n6jyu7p6sNj5Tk/7Y7v5u\n",
       "d3vt6ZjDBpy4cpads7oQ0dBe2oeN4e4dLzlV2NA7jhUTYEWm9QEEIbABghDYAEEIbIAgBDZAEAIb\n",
       "IAiBDRDE3ud0BAAAACjAGrumj7U77JoOsCIbHdnLkacIyznKmu0sFM/BnwDeWJKBuzil5Q+yG9an\n",
       "5DCaLcsdS1tXs8YeAMAceo7xWGYAAThlGLUwjgEAAAD2s9b0J/OwAQAAAACAEpmXCjtwtD4AAIBS\n",
       "aIWwNocwuEJLhLmKP3Yw1Gjpu9mtM4kMne1jzXfQ7jvyeeBr7fvD6wmHUGEzlcoaAACASkztR/av\n",
       "X0oPe+xr7ftrkdcPCvB0xm30L9vQP7kycDAbHRlz6n0HoFDaIFCg3BVz7Hp62PXov24AqxAq2/C6\n",
       "AgAslVNRXbuOlkh82iAQxNTAzg3vrQmY9UR/LaM/fshySsuPDdG9/dD/uvzu1vWW3P81azzHEq0Z\n",
       "UrW9NlClOTvOUIY1QrbGNzKolh1n2iWsAQLQ8wWgOt7cKEotZ5wxre82rwMEpofdDj1rCE5gt2Gt\n",
       "sFahw4EEdv3WrKxV6AAbURFDZWx0rFOLzxmqpiVSJxsYoUICuz7CGoBVaNXAAnrYdWjhOULTtETq\n",
       "oA0CDTh6Je8HTffY2TmOfvwlENbUqn8M/ZSMdS2RwGp9XsAAJzCIa803o+arFohAYMekDQINstEx\n",
       "HmENAAAAi12m0O3xEbuEj/KlzqroT23MvS7QkLV72KUGItdZbgAAa9ESKYOWCHCTaX0AALAmLZEy\n",
       "9A9wk3NdiKCE9b4aLbVESg3rCwObGhnTAAAAAAAAAAAwiXnYZbBrOtGUsD43p6V52MA6LmGdUwCV\n",
       "WiSFJLDLYWATxan3Pee6rEBgl8PABq4S2ABBCOyyaItQG2N6RQK7LNoi1MaYXpHABm4ZqpJVzgcQ\n",
       "2OWwAlCqoSp5yjHcWYnALoePjkRk3O5IYANLnEcuswGBDSxxGrnMBgQ2MJeKemcCG5hLRb0zgQ3k\n",
       "UE0XQGADOVTTBRDY5VDBEJFxuyOBXQ4VDFF0Q9q43ZHABqYaC2nV9sYENrAW1fbGBDZAEAIbIAiB\n",
       "DRCEwAYIQmCXxVZ2amNMr0hgl8VWdmpjTK9IYAMEIbCBuc5JywMAACAYH+MAriglJEt5HAAA1OiU\n",
       "3lSce8yV3Ot+rt1/6S7L49Z1AEKJEMA8ZJnBAmtU2N3bD/2vy+9uXW/ufedSmQKhRd9xJieEWw/q\n",
       "88hlIJjogc1tp5HLQDACGyAIgQ0QhMAGCKKmnubR87yBdhyyAb8bcEsCb+jBC0+6vKFSOtOEAQII\n",
       "MeXVjjMAALCmo6vOfqXdrcZzHP34ge30P5WntP46v9X/3YRpfUB0c/vPlzeEEGGdksAG4ssN3P5x\n",
       "dcIE9UUJx6fut0SmCPeCA6ubEr4hg/riyAp77EUL+2ICq+pXxGMFXU5mXG4fOl+0RICadT/Jhw7r\n",
       "lLREgDqFmv2R6+mB9z02fW/KtD6AlB5nRlVBfXFkYI+FsrAGLq71nbt/qzKg+/SwgaiaCOkugQ2U\n",
       "bCiUr80YAYDjHf2RYujYIY4lAqTkOPuPHLnRsat7gBcfdeoWfucFdmGcAMAcKmmgz6fsK5zTEQAA\n",
       "1jSlCr61x1HO/xs7w0z/Z7NEmMvGqnLlrNeW3RVrnIQ3VwkrUqm9sf5pkHKuC3tYY50xZgFoy5TK\n",
       "bu7/BgCAdiyZxnfr9PO3qnYVOMAEW210LGEDI8yx5YZp6wSLrHEskZxqG/a0JHSNXYq11sGfukFd\n",
       "xdmJmSV3uW89vdLYo1p7z00udS40+7D8AYC6bbHRUR+7DUPL2UHAYENLz+no4y05zp0vYKYlGx1t\n",
       "WCSHcQIrmlP1nDMu939363pzdSu3sUpOZbfcOamU4VBzKuxSK6axkC7xsUZT6jKHpkztYVtx22SZ\n",
       "QwGmVNjCun7dTymWNRQmN7CFdd20jyCA3MC2IpdnzQ1/li8EsNaxRNiXTzzQoKU7zrA/YQ2Nih7Y\n",
       "OW2B2uYMC2to1JErv7MxA0zgFGEAAABAg/4PEAxattBwFN0AAAAASUVORK5CYII=\n",
       "\" transform=\"translate(126, 12)\"/>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pkg.add(\"Plots\")\n",
    "using Plots\n",
    "\n",
    "spy(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(4)\n",
    "\n",
    "Set the _teleportation_ parameter at $p = 0.85$. Try the following methods to solve for $\\mathbf{x}$ using the `ucla.zip` data.\n",
    "\n",
    "0. A dense linear system solver such as LU decomposition.  \n",
    "0. A simple iterative linear system solver such as Jacobi or Gauss-Seidel.   \n",
    "0. A dense eigen-solver.  \n",
    "0. A simple iterative eigen-solver such as the power method.  \n",
    "\n",
    "For iterative methods, you can use the [`IterativeSolvers.jl`](https://github.com/JuliaMath/IterativeSolvers.jl) package. Make sure to utilize the special structure of $\\mathbf{P}$ (sparse + rank 1) to speed up the matrix-vector multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **LU decomposition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500×1 Array{Float64,2}:\n",
       " 0.0115263 \n",
       " 0.00118521\n",
       " 0.00368196\n",
       " 0.00361842\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00137118\n",
       " 0.00133263\n",
       " ⋮         \n",
       " 0.00136037\n",
       " 0.00136037\n",
       " 0.00129072\n",
       " 0.00337188\n",
       " 0.00215239\n",
       " 0.00136037\n",
       " 0.00136037\n",
       " 0.00750605\n",
       " 0.00136037\n",
       " 0.00136037\n",
       " 0.00136037\n",
       " 0.00107129"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# teleportation parameter \n",
    "p = 0.85\n",
    "\n",
    "# row sum of connectivity matrix A\n",
    "r = sum(A, 2);\n",
    "\n",
    "# number of pages\n",
    "n = 500\n",
    "\n",
    "# initialize the transition matrix \n",
    "P = zeros(500, 500)\n",
    "\n",
    "# input transition probability from page i to page j\n",
    "for j in 1:500\n",
    "    for i in 1:500\n",
    "        if r[i] > 0\n",
    "            P[i, j] = p * A[i, j] / r[i] + (1 - p) / n\n",
    "        else\n",
    "            P[i, j] = 1 / n\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "# adjustment for M = P'-I and b to avoid rank deficiency\n",
    "Q = P' - I\n",
    "Q[1, :] = 1.0\n",
    "\n",
    "b1 = zeros(n, 1)\n",
    "b1[1, 1] = 1.0\n",
    "\n",
    "# LU decomposition for M\n",
    "Qlu = lufact(Q, Val{true})\n",
    "\n",
    "# get the result for x \n",
    "Qlu \\ b1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Jacobi Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Decomposition of  $\\mathbf{P}$:\n",
    "$\\mathbf{P} = \\mathbf{R}\\mathbf{A} + \\mathbf{e}\\mathbf{v}^T$ where $\\mathbf{v}$ is a column vector with n ones,\n",
    "$$\n",
    "\\mathbf{R}=\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "    \\frac{p}{r_i} \\text{ on the diagonal for } r_i>0 \\\\\n",
    "    0 \\text{ otherwise. } \n",
    "    \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "\n",
    "and $\\mathbf{e}$ is a column vector with $n$ elements,\n",
    "$$\n",
    "\\mathbf{e}=\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "    \\frac{1-p}{n} \\text{ for } r_i>0 \\\\\n",
    "    \\frac{1}{n} \\text{ for } r_i=0\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "* In this question, we want to solve $(P^T-I)x=0$ with iterative method Jacobi method.\n",
    "* First, split $P^T-I= L+D+U$, where $D$ is the diaganol of $P^T-I$, $U$ is the upper part without the diagonal of $P^T-I$, and $L$ is the lower part without diagonal of $P^T-I$.\n",
    "* Jacobi iteration can be written as \n",
    "$$\n",
    "\\begin{align}\n",
    "x^{(t+1)} \n",
    "&= -D^{-1}(P^T-I)x^{(t)}+x^{(t)}+D^{-1}b\\\\\n",
    "&= -D^{-1}((R*A+ev^T)^T-I)x^{(t)}+D^{-1}b+x^{(t)}\\\\\n",
    "&= -D^{-1}((A^TR^T+ve^T-I)x^{(t)}+D^{-1}b+x^{(t)}\\\\\n",
    "&= -D^{-1}A^TR^Tx^{(t)}-D^{-1}ve^Tx^{(t)}+D^{-1}Ix^{(t)}+D^{-1}b+x^{(t)}\\\\\n",
    "&= -D^{-1}A^TR^Tx^{(t)}-D^{-1}ve^Tx^{(t)}+D^{-1}x^{(t)}+x^{(t)} \\space with\\space b = 0\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500×1 Array{Float64,2}:\n",
       " 0.0115263 \n",
       " 0.00118521\n",
       " 0.00368196\n",
       " 0.00361842\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00137118\n",
       " 0.00133263\n",
       " ⋮         \n",
       " 0.00136037\n",
       " 0.00136037\n",
       " 0.00129072\n",
       " 0.00337188\n",
       " 0.00215239\n",
       " 0.00136037\n",
       " 0.00136037\n",
       " 0.00750605\n",
       " 0.00136037\n",
       " 0.00136037\n",
       " 0.00136037\n",
       " 0.00107129"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# P = RA + ev'\n",
    "p = 0.85\n",
    "n = 500\n",
    "rowsum = sum(A, 2)\n",
    "\n",
    "# intialization\n",
    "v = ones(n)\n",
    "R_diag = zeros(n, 1)\n",
    "e = zeros(n, 1)\n",
    "\n",
    "#input e and R_diag\n",
    "for i in 1:n\n",
    "    if rowsum[i] > 0\n",
    "        R_diag[i] = p / rowsum[i]\n",
    "        e[i] = (1 - p) / n\n",
    "    else\n",
    "        R_diag[i] = 0.0   \n",
    "        e[i] = 1 / n\n",
    "    end\n",
    "end\n",
    "\n",
    "# initial guess and initialize x\n",
    "x_old = ones(n, 1) \n",
    "x_new = zeros(n, 1)\n",
    "\n",
    "# D inverse\n",
    "D_inv = inv(Diagonal(P' - I))\n",
    "\n",
    "for i in 1:1000\n",
    "    \n",
    "    x_new .= A_mul_B!(x_new, (-D_inv * A'), (R_diag .* x_old)) - \n",
    "        diag(D_inv) .* v * dot(e, x_old) + diag(D_inv) .* x_old + x_old \n",
    "\n",
    "    if vecnorm(x_new - x_old) < 1e-5\n",
    "        break\n",
    "    end\n",
    "    x_old = copy(x_new)\n",
    "end\n",
    "\n",
    "x_new / sum(x_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **A dense eigen-solver**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Solve for $P^Tx = x$ is same as solve for $P^Tx = \\lambda x$ with $\\lambda = 1$. Therefore, we are looking for the eigenvector corresponding to $\\lambda = 1$ and standardize the eigenvector by making the sum of each element to one (each element in x represents the proportion of time visitor visit each page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500-element Array{Float64,1}:\n",
       " 0.0115263 \n",
       " 0.00118521\n",
       " 0.00368196\n",
       " 0.00361842\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00118521\n",
       " 0.00137118\n",
       " 0.00133263\n",
       " ⋮         \n",
       " 0.00136037\n",
       " 0.00136037\n",
       " 0.00129072\n",
       " 0.00337188\n",
       " 0.00215239\n",
       " 0.00136037\n",
       " 0.00136037\n",
       " 0.00750605\n",
       " 0.00136037\n",
       " 0.00136037\n",
       " 0.00136037\n",
       " 0.00107129"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eigen factor the matrix P'\n",
    "P_eigen = eigfact(P')\n",
    "\n",
    "# check the lambda = 1\n",
    "P_eigen[:values]\n",
    "\n",
    "# find the corresponding eigenvector\n",
    "x_3 = P_eigen[:vectors][:, 1]\n",
    "\n",
    "# normalize the vector\n",
    "x_3 = x_3 / sum(x_3)\n",
    "\n",
    "# converet the complex array to float64 array\n",
    "Float64.(x_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Power Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the iterative method, the update is followed by the following forumula,$$X^{(t)}\\leftarrow \\frac{1}{\\|Ax^{(t-1)}\\|_2}Ax^{(t-1)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500-element Array{Float64,1}:\n",
       " 0.0115264 \n",
       " 0.00118522\n",
       " 0.00368199\n",
       " 0.00361845\n",
       " 0.00118522\n",
       " 0.00118522\n",
       " 0.00118522\n",
       " 0.00118522\n",
       " 0.00118522\n",
       " 0.00118522\n",
       " 0.00118522\n",
       " 0.00137119\n",
       " 0.00133264\n",
       " ⋮         \n",
       " 0.00136038\n",
       " 0.00136038\n",
       " 0.00129073\n",
       " 0.0033719 \n",
       " 0.00215241\n",
       " 0.00136038\n",
       " 0.00136038\n",
       " 0.00750612\n",
       " 0.00136038\n",
       " 0.00136038\n",
       " 0.00136038\n",
       " 0.0010713 "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial guess\n",
    "x_old = 1 /  n * ones(n)\n",
    "\n",
    "# tolerence\n",
    "tol = 1e-5\n",
    "\n",
    "# initilization \n",
    "x_new = zeros(n)\n",
    "\n",
    "for i in 1:1000\n",
    "    x_new = 1 / vecnorm(P' * x_old) * P' * x_old\n",
    "    \n",
    "    if vecnorm(x_new - x_old) < tol\n",
    "        break\n",
    "    end\n",
    "    \n",
    "    x_old = copy(x_new)\n",
    "end\n",
    "\n",
    "x_new = x_new / sum(x_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(5)\n",
    "\n",
    "List the top 20 ranked URLs you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20-element Array{Any,1}:\n",
       " \"http://hammer.ucla.edu\"                                                                \n",
       " \"http://schema.org\"                                                                     \n",
       " \"http://www\"                                                                            \n",
       " \"http://giveto.ucla.edu/comments/feed\"                                                  \n",
       " \"http://browsehappy.com\"                                                                \n",
       " \"http://schema.org/Text\"                                                                \n",
       " \"http://www.ucla.edu\"                                                                   \n",
       " \"http://www.directory.ucla.edu\"                                                         \n",
       " \"http://www.universityofcalifornia.edu\"                                                 \n",
       " \"http://schema.org/CreativeWork\"                                                        \n",
       " \"http://www.registrar.ucla.edu/calendar\"                                                \n",
       " \"http://giveto.ucla.edu\"                                                                \n",
       " \"http://www.uclalumni.net\"                                                              \n",
       " \"http://www.magazine.ucla.edu/features/get-the-picture-the-ucla-brain-mapping-center\"   \n",
       " \"http://www.magazine.ucla.edu/features/heads-up\"                                        \n",
       " \"http://www.magazine.ucla.edu/features/hope-is-real-the-ucla-depression-grand-challenge\"\n",
       " \"http://www.magazine.ucla.edu/exclusives/brotherly-love\"                                \n",
       " \"http://www.magazine.ucla.edu/features/patient-test-thyself\"                            \n",
       " \"http://www.magazine.ucla.edu/depts/style/the-dancing-scientist\"                        \n",
       " \"http://www.magazine.ucla.edu/features/splendor-in-the-trash\"                           "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fint the top 20 URL index\n",
    "\n",
    "rank = sortperm(x_new, rev = true)[1:20]\n",
    "\n",
    "# find the matching URL in U\n",
    "U[rank, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(6)\n",
    "\n",
    "As of Monday May 11 2018, there are at least 1.83 billion indexed webpages on internet according to <http://www.worldwidewebsize.com/>. Explain whether each of these methods works for the PageRank problem at this scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "* Both LU decomposition and dense eigen-solver are direct methods, which are only good for small to moderate matrices. Take LU decomposition as an example, it takes $\\frac{2}{3}n^3$ flops to solve for a linear system. When n is at least 1.83 billion, it takes forever to solve for the linear system related to huge amount of webpages. Therefore, the direct methods will not solve for the PageRank problem at large scale.\n",
    "* Both Jacobi method and power method are iterative methods, which are good for large scale problem. In Jacobi method, one round costs $2n^2$ flops, which is much better than direct methods. In the PageRank problem, the matrix $P$ is decompsed to sparse and low rank matrices, which even further optimize the problem. In power method, each iteration costs $O(n)$ flops, which is also great for large scale problem.\n",
    "* Also, if the iterative methods start with a better initial guess, they can probably get closer to the true solution with fast convergence. In the PageRank solution, this means the previous data can be used as an initial guess for solving later problems. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
