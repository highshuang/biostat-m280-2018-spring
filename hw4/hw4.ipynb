{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M280 Homework4\n",
    "* Author: Shuang Gao\n",
    "* Date: 2018/06/01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "For a multivariate count vector $\\mathbf{x}=(x_1,\\ldots,x_d)$ with batch size $|\\mathbf{x}|=\\sum_{j=1}^d x_j$, show that the probability mass function for Dirichlet-multinomial distribution is\n",
    "$$\n",
    "    f(\\mathbf{x} \\mid \\alpha) \n",
    "\t= \\int_{\\Delta_d} \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\pi(\\mathbf{p}) \\, d \\mathbf{p}  \n",
    "    = \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}\n",
    "$$\n",
    "where $\\Delta_d$ is the unit simplex in $d$ dimensions and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\mathbf{x} \\mid \\alpha) \n",
    "&= \\int_{\\Delta_d} \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\pi(\\mathbf{p}) \\, d \\mathbf{p} \\\\\n",
    "&= \\int_{\\Delta_d} \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1}\\,d \\mathbf{p}\\\\\n",
    "&= \\binom{|\\mathbf{x}|}{\\mathbf{x}} \n",
    "\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\int_{\\Delta_d} \\prod_{j=1}^d p_j^{x_j+\\alpha_j-1}\\, d\\mathbf{p}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Because $\\mathbf{p} = (p_1,\\ldots, p_d)$ follow a Dirichlet distribution with parameter vector $\\alpha = (\\alpha_1,\\ldots, \\alpha_d)$, $\\alpha_j>0$, and density\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\pi(\\mathbf{p}) =  \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1},\n",
    "\\end{eqnarray*} \n",
    "$$\n",
    "where $|\\alpha|=\\sum_{j=1}^d \\alpha_j$.\n",
    "We can get,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\int \\pi(\\mathbf{p})\\,d\\mathbf{p} \n",
    "&= \\int \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\,d\\mathbf{p}=1\\\\\n",
    "& \\therefore \\int \\prod_{j=1}^d p_j^{\\alpha_j-1}\\,d\\mathbf{p}=\\frac{\\prod_{j=1}^d\\Gamma{(\\alpha_j})}{\\Gamma{(|\\alpha|)}}\\hspace{12mm}(1)\n",
    "\\end{align}\n",
    "$$\n",
    "In order to get the derivation of $f(\\mathbf{x} \\mid \\alpha) $, Plug $\\alpha_j=\\alpha_j+x_j$ into equation (1).\n",
    "Then we have,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\int \\prod_{j=1}^d p_j^{\\alpha_j+x_j-1}\\,d\\mathbf{p}=\\frac{\\prod_{j=1}^d\\Gamma{(\\alpha_j}+x_j)}{\\Gamma{(|\\alpha|+|\\mathbf{x}|)}}\n",
    "\\end{align}\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\mathbf{x} \\mid \\alpha) \n",
    "&= \\binom{|\\mathbf{x}|}{\\mathbf{x}} \n",
    "\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\prod_{j=1}^d\\Gamma{(\\alpha_j}+x_j)}{\\Gamma{(|\\alpha|+|\\mathbf{x}|)}}\\\\\n",
    "&=  \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Given independent data points $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$, show that the log-likelihood is\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)]\n",
    "$$\n",
    "Is the log-likelihood a concave function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\because \\mathbf{x_1},...,\\mathbf{x_n} \\text{ are independent}\\\\ \n",
    "& \\therefore f(\\mathbf{x_1},...,\\mathbf{x_n} \\mid \\alpha) \n",
    "= f(\\mathbf{x_1}\\mid \\alpha)...f(\\mathbf{x_n\\mid \\alpha})\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\therefore f(\\mathbf{x_1},...,\\mathbf{x_n} \\mid \\alpha) \n",
    "= \\prod_{i=1}^n \\binom{|\\mathbf{x_i}|}{\\mathbf{x_i}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_{ij})}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x_i}|)}\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\therefore L(\\alpha) \n",
    "&= \\ln (\\prod_{i=1}^n \\binom{|\\mathbf{x_i}|}{\\mathbf{x_i}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_{ij})}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x_i}|)})\\\\\n",
    "&= \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* The function is not concave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Write Julia function to compute the log-density of the Dirichlet-multinomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log density of the Dirichlet-multinomial distribution for one data point $\\mathbf{x}$ is shown as follow:\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\alpha) \n",
    "&= \\ln \\binom{|\\mathbf{x}|}{\\mathbf{x}} +  \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{j}) - \\ln \\Gamma(\\alpha_j)] - [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}|) - \\ln \\Gamma(|\\alpha|)]\\\\\n",
    "&=  \\ln |\\mathbf{x}|! - \\ln(x_1!)-...-\\ln(x_{64}!)+ \\sum_{j=1}^{64} [\\ln \\Gamma(\\alpha_j + x_{j}) - \\ln \\Gamma(\\alpha_j)] - [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}|) - \\ln \\Gamma(|\\alpha|)]\\\\\n",
    "&=  \\ln |\\mathbf{x}|! - [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}|) - \\ln \\Gamma(|\\alpha|)]+ \\sum_{j=1}^{64} [\\ln \\Gamma(\\alpha_j + x_{j}) - \\ln \\Gamma(\\alpha_j)-\\ln(x_j!)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_logpdf(x::Vector, α::Vector)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at data point `x`.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(x::Vector, α::Vector)\n",
    "    # store the sum of x and α\n",
    "    sumx = sum(x)\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    # store length of x\n",
    "    lengthx = size(x, 1) \n",
    "    \n",
    "    # part without summation\n",
    "    logden = lfact(sumx) - lgamma(sumα + sumx) + lgamma(sumα)\n",
    "    \n",
    "    # part within summation\n",
    "    for j in lengthx\n",
    "       logden += lgamma(α[j] + x[j]) - lgamma(α[j]) - lfact(x[j])  \n",
    "    end\n",
    "    \n",
    "    # return log density\n",
    "    return logden\n",
    "end\n",
    "\n",
    "function dirmult_logpdf!(r::Vector, X::Matrix, α::Vector)\n",
    "    for j in 1:size(X, 2)\n",
    "        r[j] = dirmult_logpdf(X[:, j], α)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at each data point in `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::Matrix, α::Vector)\n",
    "    r = zeros(size(X, 2))\n",
    "    dirmult_logpdf!(r, X, α)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Read in `optdigits.tra`, the training set of 3823 handwritten digits. Each row contains the 64 counts of a digit and the last element (65th element) indicates what digit it is. For grading purpose, evaluate the total log-likelihood of this data at parameter values $\\alpha=(1,\\ldots,1)$ using your function in Q3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in train data\n",
    "path1_tra = \"http://hua-zhou.github.io/teaching/biostatm280\"\n",
    "path2_tra = \"-2018spring/hw/hw4/optdigits.tra\"\n",
    "\n",
    "train = readdlm(download(path1_tra * path2_tra), ',')\n",
    "\n",
    "# separte last column which store the digit \n",
    "train_x = round.(Int64, train[:, 1:64])\n",
    "train_digit = round.(Int64, train[:, 65]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-638817.993292528"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store size of train data\n",
    "length_x = size(train_x, 2)\n",
    "length_obs = size(train_x, 1)\n",
    "\n",
    "# input α vector\n",
    "α = round.(Int64, ones(length_x))\n",
    "\n",
    "sum(dirmult_logpdf(train_x', α))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The total log-likelihood of this data at parameter values $\\alpha=(1,\\ldots,1)$ is around -638818."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question5 \n",
    "Derive the score function $\\nabla L(\\alpha)$, observed information matrix $-d^2L(\\alpha)$, and Fisher information matrix $\\mathbf{E}[-d^2L(\\alpha)]$ for the Dirichlet-multinomial distribution.\n",
    "\n",
    "Comment on why Fisher scoring method is inefficient for computing MLE in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "* **score function**\n",
    "\n",
    "From part 2, we have \n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)]\n",
    "$$\n",
    "By taking first derivative of $L(\\alpha)$ respect to each element in vector $\\alpha$, we get the socre function to be as follow,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla L(\\alpha) \n",
    "&= \\sum_{i = 1}^n (\\Psi(\\alpha_j+x_{ij})-\\Psi(\\alpha_j)) - \\sum_{i=1}^n(\\Psi(|\\alpha|+|x_i|)-\\Psi(|\\alpha|))\\\\\n",
    "&= \\sum_{i = 1}^n (\\Psi(\\alpha_j+x_{ij})-\\Psi(\\alpha_j)) - \\sum_{i=1}^n\\Psi(|\\alpha|+|x_i|)+n\\Psi(|\\alpha|)\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\Psi(x)= \\frac{d}{dx}log\\Gamma(x)$ is the digamma function.\n",
    "* **observed information matrix**\n",
    "\n",
    "$$\n",
    "-d^2L(\\alpha)=-\\frac{d\\nabla L(\\alpha)}{d\\alpha}\n",
    "=\\sum_{i = 1}^n[\\Psi'(\\alpha_j)-\\Psi'(\\alpha_j+x_{ij})]-\\sum_{i = 1}^n[\\Psi'(|\\alpha|)-\\Psi'(|\\alpha|+|x_i|)]\n",
    "$$\n",
    "(1) diagonal matrix part, $D$\n",
    "\n",
    "When taking the second derivative respect to $\\alpha$, the terms with only $\\alpha_j$ will pretain if both of the derivatives taken respect to $\\alpha_j$. That is $\\Psi'(\\alpha_j)$ and $\\Psi'(\\alpha_j+x_{ij})$ terms will only appear on the diaganol of observed information matrix. Therefore, let's define a diagonal matrix $D$, which has the $j^{th}$diagonal entry shown as\n",
    "$\\sum_{i = 1}^n[\\Psi'(\\alpha_j)-\\Psi'(\\alpha_j+x_{ij})]$.\n",
    "From recurrence relation of polygamma function $\\Psi^{(m)}(z+1)=\\Psi^{(m)}(z)+\\frac{(-1)^mm!}{z^{m+1}}$, the trigamma function$\\Psi'(z+1)=\\Psi^{(1)}(z+1)=\\Psi^{(1)}(z)+\\frac{(-1)^mm!}{z^{m+1}}$.\n",
    "\n",
    "Then,\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\Psi^{(1)}(\\alpha_j+1)=\\Psi^{(1)}(\\alpha_j)-\\frac{1}{\\alpha_j^2}\\\\\n",
    "&\\Psi^{(1)}(\\alpha_j+2)=\\Psi^{(1)}(\\alpha_j+1)-\\frac{1}{\\alpha_j^2+1}\\\\\n",
    "&.\\\\\n",
    "&.\\\\\n",
    "&.\\\\\n",
    "&\\Psi^{(1)}(\\alpha_j+x_{ij})=\\Psi^{(1)}(\\alpha_j+x_{ij}-1)-\\frac{1}{(\\alpha_j+x_{ij}-1)^2}\n",
    "\\end{align}\n",
    "$$\n",
    "By suming all the above equations, we get\n",
    "$\\Psi'(\\alpha_j)-\\Psi'(\\alpha_j+x_{ij})=\\sum_{p=0}^{x_{ij}-1} \\frac{1}{(\\alpha_j+p)^2}$\n",
    "Therefore, the $j_{th}$ diagonal position in matrix $D$, \n",
    "$$\\sum_{i = 1}^n[\\Psi'(\\alpha_j)-\\Psi'(\\alpha_j+x_{ij})]=\\sum_{i = 1}^n\\sum_{p=0}^{x_{ij}-1} \\frac{1}{(\\alpha_j+p)^2}$$\n",
    "(2) other part, $m$\n",
    "\n",
    "Similarly, the terms left in the observated information matrix is \n",
    "$$\n",
    "m= \\sum_{i = 1}^n[\\Psi'(|\\alpha|)-\\Psi'(|\\alpha|+|x_i|)]\n",
    "= \\sum_{i = 1}^n\\sum_{p=0}^{|x_{i}|-1} \\frac{1}{(|\\alpha|+p)^2}\n",
    ",$$ which is a constant over the whole matrix.\n",
    "\n",
    "Therefore, the observed info matrix can be written as \n",
    "$\\mathbf{D}-m\\mathbf{1}\\mathbf{1}'$, where $\\mathbf{D}$ and $m$ are defined previously.\n",
    "\n",
    "* **Fisher information matrix $\\mathbf{E}[-d^2L(\\alpha)]$ **\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{E}[-d^2L(\\alpha)]\n",
    "&=\\mathbf{E}(\\mathbf{D}-m\\mathbf{1}\\mathbf{1}')\\\\\n",
    "&=\\mathbf{E}(\\mathbf{D})-m\\mathbf{1}\\mathbf{1}'\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "For calculating the expected value, we need to integrate over $\\alpha$, which does not have analytical solution. Therefore, Fisher scoring method is insufficient for computing MLE in this example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What structure does the observed information matrix possess that can facilitate the evaluation of the Newton direction? Is the observed information matrix always positive definite? What remedy can we take if it fails to be positive definite? (Hint: HW1 Q6.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Solution**\n",
    "\n",
    "1.special structure\n",
    "\n",
    "From part 5, we can see the observed information matrix $[-d^2L(\\alpha)]=\\mathbf{D}-m\\mathbf{1}\\mathbf{1}'$, where $D$ is a diagonal matrix with $j^{th}$ diagonal entry, $\\sum_{i = 1}^n\\sum_{p=0}^{x_{ij}-1} \\frac{1}{(\\alpha_j+p)^2}$ and $m=\\sum_{i = 1}^n\\sum_{p=0}^{|x_{i}|-1} \\frac{1}{(|\\alpha|+p)^2}$ is a constant. Therefore the observed information matrix is composed by a diagonal matrix and a rank one matrix. \n",
    "\n",
    "2.not always positive difinite \n",
    "$$\n",
    "\\begin{align}\n",
    "det[-d^2L(\\alpha)] \n",
    "&= det[\\mathbf{D}-m\\mathbf{1}\\mathbf{1}'] \\\\\n",
    "&= det[\\mathbf{D}(\\mathbf{I}-\\mathbf{D}^{-1}m\\mathbf{1}\\mathbf{1}')] \\\\\n",
    "&= det(\\mathbf{D})det(\\mathbf{I}-m\\mathbf{D}^{-1}\\mathbf{1}\\mathbf{1}')\\\\\n",
    "&= (\\prod_{j}d_j)(1-\\sum_j\\frac{m}{d_j})\n",
    "\\end{align}\n",
    "$$\n",
    "In order to make the matrix positive definite, the determant of the matrix should be positive. From the above equation, that is $1-\\sum_j\\frac{m}{d_j}>0$ should be satisfied. However, it is not always the case. Therefore, the matrix is not necessarily positive definite.\n",
    "\n",
    "3.remedy for pd\n",
    "\n",
    "In order to make the matrix positive definite, $1-\\sum_j\\frac{m}{d_j}>0$ should be satisfied. That is, $m<\\frac{1}{\\sum_j(dj^{-1})}$\n",
    "\n",
    "Therefore, we can take the remedy according to the following rules,\n",
    "$$\n",
    "m_{new} = \n",
    "\\begin{cases}\n",
    "m & \\text{if } m<\\frac{1}{\\sum_j(dj^{-1})}\\\\\n",
    "0.9 \\frac{1}{\\sum_j(dj^{-1})}& \\text{o.w.}  \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "4.facillitate the evaluation of the Newton direction.\n",
    "\n",
    "\n",
    "With the Sherman-Morrison formula, $$\n",
    "\t(\\mathbf{A} + \\mathbf{u} \\mathbf{u}^T)^{-1} = \\mathbf{A}^{-1} - \\frac{1}{1 + \\mathbf{u}^T \\mathbf{A}^{-1} \\mathbf{u}} \\mathbf{A}^{-1} \\mathbf{u} \\mathbf{u}^T \\mathbf{A}^{-1},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "[-d^2L(\\alpha)]^{-1}\n",
    "&= [\\mathbf{D}-m\\mathbf{1}\\mathbf{1}']^{-1}\\\\\n",
    "&= \\mathbf{D}^{-1}+\\frac{m}{1+m\\mathbf{1}'\\mathbf{D}^{-1}\\mathbf{1}}\\mathbf{D}^{-1}\\mathbf{1}\\mathbf{1}'\\mathbf{D}^{-1}\\\\\n",
    "&= \\mathbf{D}^{-1}+\\frac{1}{m^{-1}+\\mathbf{1}'\\mathbf{D}^{-1}\\mathbf{1}}\\mathbf{D}^{-1}\\mathbf{1}\\mathbf{1}'\\mathbf{D}^{-1}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In this way, the inverse of observed info matrix can be decomposed to calculate the inverse of a diagonal matrix $\\mathbf{D}$. In this way, the evaluation of the Newton direction can be facillitated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Discuss how to choose a good starting point. Implement this as the default starting value in your function below. (Hint: Method of moment estimator may furnish a good starting point.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Solution**\n",
    "\n",
    "From the Dirichlet distruibution, we have the first and second theoritical moments shown as below,\n",
    "$$\n",
    "\\mathbf{E}(p_k)=\\frac{\\alpha_k}{|\\alpha|}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{E}(p_k^2)=\\frac{\\alpha_k(\\alpha_k+1)}{|\\alpha|(1+|\\alpha|)}\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\sum_k\\frac{\\mathbf{E}(p_j^2)}{\\mathbf{E}(p_j)} \n",
    "= \\sum_k\\frac{\\frac{\\alpha_k(\\alpha_k+1)}{|\\alpha|(1+|\\alpha|)}}{\\frac{\\alpha_k}{|\\alpha|}}\n",
    "= \\sum_k\\frac{\\alpha_k+1}{1+|\\alpha|}\n",
    "= \\frac{|\\alpha|+d}{|\\alpha|+1}\n",
    "$$\n",
    "From the n observations, we have the first and second sample moments shown as below,\n",
    "$$\n",
    "\\mathbf{E}(p_k)=\\frac{1}{n}\\sum_{i=1}^n\\frac{x_{ik}}{|x_i|}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{E}(p_k^2)=\\frac{1}{n}\\sum_{i=1}^n(\\frac{x_{ik}}{|x_i|})^2\n",
    "$$\n",
    "then let constant c defined as follow,\n",
    "$$\n",
    "c=\\sum_k\\frac{\\mathbf{E}(p_j^2)}{\\mathbf{E}(p_j)} \n",
    "=\\sum_k\\frac{\\frac{1}{n}\\sum_{i=1}^n(\\frac{x_{ik}}{|x_i|})^2}{\\frac{1}{n}\\sum_{i=1}^n\\frac{x_{ik}}{|x_i|}}\n",
    "=\\sum_k\\frac{\\sum_{i=1}^n(\\frac{x_{ik}}{|x_i|})^2}{\\sum_{i=1}^n\\frac{x_{ik}}{|x_i|}}\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "\\frac{|\\alpha|+d}{|\\alpha|+1}=c\\\\\n",
    "(c+1)|\\alpha|=d-c\\\\\n",
    "|\\alpha|=\\frac{d-c}{c-1}\n",
    "$$\n",
    "In order to get each elements in $\\alpha$, \n",
    "$$\n",
    "\\alpha_k = |\\alpha|\\mathbf{E}(p_k)=\\frac{d-c}{c-1}\\frac{\\sum_i x_{ik}}{\\sum_i|\\mathbf{x_i}|}\n",
    "$$\n",
    "where $c=\\sum_k\\frac{\\sum_{i=1}^n(\\frac{x_{ik}}{|x_i|})^2}{\\sum_{i=1}^n\\frac{x_{ik}}{|x_i|}}$\n",
    "\n",
    "In this way, we can use the given data $x_{ik}$ where $i$ from 1 to number of all data points and $k$ is from 1 to $d=64$ to calculate the initial $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using the Newton's method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **function to calculate score**\n",
    "\n",
    "$$\n",
    "\\nabla L(\\alpha) \n",
    "= \\sum_{i = 1}^n (\\Psi(\\alpha_j+x_{ij})-\\Psi(\\alpha_j)) - \\sum_{i=1}^n(\\Psi(|\\alpha|+|x_i|)-\\Psi(|\\alpha|))\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_score (generic function with 2 methods)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pkg.add(\"SpecialFunctions.jl\")\n",
    "using SpecialFunctions\n",
    "\n",
    "# define the function to calculate score function of log-likelihood\n",
    "# take column of X as one data point\n",
    "function dirmult_score(X::Matrix, α::Vector)\n",
    "    # initialize output \n",
    "    score = zeros(length(α))\n",
    "    \n",
    "    # sum of alpha vector\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    # column sum of X\n",
    "    colsumX = sum(X, 1)\n",
    "    \n",
    "    # store the size of n and d\n",
    "    n = size(X, 2)\n",
    "    d = size(X, 1)\n",
    "    \n",
    "    # evalutate score function at input α\n",
    "    for j in 1:d\n",
    "        for i in 1:n\n",
    "            score[j] += digamma(α[j] + X[j, i]) - digamma(α[j]) - \n",
    "                digamma(sumα + colsumX[i]) + digamma(sumα)\n",
    "            end \n",
    "    end\n",
    "    \n",
    "    # return vector score\n",
    "    return score \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64-element Array{Float64,1}:\n",
       " -6809.0  \n",
       " -5994.02 \n",
       "   399.53 \n",
       "  4465.77 \n",
       "  4255.45 \n",
       "  -239.022\n",
       " -4871.03 \n",
       " -6583.75 \n",
       " -6804.21 \n",
       " -3509.27 \n",
       "  3477.72 \n",
       "  4619.83 \n",
       "  3901.48 \n",
       "     ⋮    \n",
       "  3294.76 \n",
       "  2375.96 \n",
       " -2036.58 \n",
       " -6477.55 \n",
       " -6808.0  \n",
       " -6101.94 \n",
       "   525.922\n",
       "  4463.0  \n",
       "  3988.35 \n",
       "   655.835\n",
       " -3961.5  \n",
       " -6442.54 "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for score function\n",
    "dirmult_score(train_x', α)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **function to calculate observed info matrix**\n",
    "\n",
    "The observed info matrix can be written as $\\mathbf{D}-m\\mathbf{1}\\mathbf{1}'$, where $\\mathbf{D}$ and $m$ are defined as follow:\n",
    "\n",
    "$j_{th}$ diagonal position in matrix $D$,\n",
    "$$d_j=\\sum_{i = 1}^n[\\Psi'(\\alpha_j)-\\Psi'(\\alpha_j+x_{ij})]$$\n",
    "and \n",
    "$$\n",
    "m= \\sum_{i = 1}^n[\\Psi'(|\\alpha|)-\\Psi'(|\\alpha|+|x_i|)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_obs (generic function with 3 methods)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to calculate the observed information matrix \n",
    "# return diagonal vector d and constant m for recovery\n",
    "# take column of X as one data point\n",
    "function dirmult_obs(X::Matrix, α::Vector)\n",
    "    # obs matrix = Diagonal(d) - m11'\n",
    "    # initialization \n",
    "    d = zeros(length(α))\n",
    "    m = 0.0\n",
    "    \n",
    "    # store the size of n and number of parameters\n",
    "    n = size(X, 2)\n",
    "    par = size(X, 1)\n",
    "    \n",
    "    # sum of alpha vector\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    # column sum of X\n",
    "    colsumX = sum(X, 1)\n",
    "    \n",
    "    # evaluation d \n",
    "    for j in 1:par\n",
    "        for i in 1:n\n",
    "            d[j] += trigamma(α[j]) - trigamma(α[j] + X[j, i])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # evaluation m\n",
    "    for i in 1:n\n",
    "        m += trigamma(sumα) - trigamma(sumα + colsumX[i])\n",
    "    end\n",
    "    \n",
    "    return m, d\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50.0163869748493, [0.0, 688.915, 4313.97, 5730.92, 5665.9, 3860.62, 1242.08, 151.565, 3.71361, 2231.94  …  2853.36, 255.11, 1.0, 574.829, 4306.32, 5688.55, 5469.75, 4162.56, 1787.01, 264.045])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for obs matrix fcn\n",
    "dirmult_obs(train_x', α)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **initial $\\mathbf{α}$**\n",
    "\n",
    "The $k^{th}$ element in initial $\\mathbf{α}$ is \n",
    "$$\n",
    "\\alpha_k = \\frac{d-c}{c-1}\\frac{\\sum_i x_{ik}}{\\sum_i|\\mathbf{x_i}|}\n",
    "$$\n",
    "where $c=\\sum_k\\frac{\\sum_{i=1}^n(\\frac{x_{ik}}{|x_i|})^2}{\\sum_{i=1}^n\\frac{x_{ik}}{|x_i|}}$\n",
    "and \n",
    "$d$ is the length of $\\mathbf{α}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_α0 (generic function with 2 methods)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to calculate the initial value for α\n",
    "# take column of X as one data point\n",
    "function dirmult_α0(X::Matrix)\n",
    "    # initialization\n",
    "    α0 = zeros(length(α))\n",
    "    c = 0.0\n",
    "    \n",
    "    # store the size of n and number of parameters \n",
    "    n = size(X, 2)\n",
    "    d = size(X, 1)\n",
    "    \n",
    "    # sum of alpha vector\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    # column sum of X\n",
    "    colsumX = sum(X, 1)\n",
    "    \n",
    "    # total of X\n",
    "    total = sum(colsumX)\n",
    "    \n",
    "    # evaluate c \n",
    "    for k in 1:d\n",
    "        denumerator = sum(X[k, :] ./ colsumX)\n",
    "        if  denumerator > 0\n",
    "            c += sum(X[k, :].^2 ./ colsumX.^2) / denumerator \n",
    "        end\n",
    "    end\n",
    "    \n",
    "        \n",
    "    # evaluate α0 and make sure αsum is nonegative\n",
    "    α0 = max((d - c) / (c - 1), 1e-4) / total * sum(X, 2)\n",
    "    \n",
    "    return α0\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×1 Array{Float64,2}:\n",
       " 0.0        \n",
       " 0.0683308  \n",
       " 1.24306    \n",
       " 2.67712    \n",
       " 2.59675    \n",
       " 1.2484     \n",
       " 0.314606   \n",
       " 0.0322673  \n",
       " 0.000474519\n",
       " 0.444565   \n",
       " 2.39852    \n",
       " 2.6566     \n",
       " 2.40931    \n",
       " ⋮          \n",
       " 2.21428    \n",
       " 2.10509    \n",
       " 0.848974   \n",
       " 0.0336316  \n",
       " 5.93149e-5 \n",
       " 0.0641787  \n",
       " 1.32788    \n",
       " 2.7082     \n",
       " 2.59894    \n",
       " 1.51941    \n",
       " 0.477485   \n",
       " 0.0458504  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for initial alpha\n",
    "dirmult_α0(train_x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **implement of Newton's method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_newton"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_newton(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using Newton's method.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `n`-by-`d` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: a `d` vector of starting point (optional). \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "* `maximum`: the log-likelihood at MLE.   \n",
    "* `estimate`: the MLE. \n",
    "* `gradient`: the gradient at MLE. \n",
    "* `hessian`: the Hessian at MLE. \n",
    "* `se`: a `d` vector of standard errors. \n",
    "* `iterations`: the number of iterations performed.\n",
    "\"\"\"\n",
    "\n",
    "function dirmult_newton(X::Matrix; α0::Vector = nothing, \n",
    "            maxiters::Int = 100, tolfun::Float64 = 1e-6)\n",
    "    # store original size of α\n",
    "    α_d = size(X, 1)\n",
    "    \n",
    "    # remove rows with all 0 entries\n",
    "    row_ind = find(sum(X, 2))\n",
    "    col_ind = find(sum(X, 1))\n",
    "    \n",
    "    # get the new X removing all the 0 sum lines\n",
    "    X_nz = X[row_ind, :]\n",
    "    \n",
    "    # store the number of parameters \n",
    "    d = size(X_nz, 1)\n",
    "    \n",
    "    # initialization\n",
    "    α_old = α0[row_ind]\n",
    "    logl_old = sum(dirmult_logpdf(X_nz, α_old))\n",
    "    \n",
    "    # pre-allocation \n",
    "    α_new = zeros(d)\n",
    "    gradient = zeros(d)\n",
    "    \n",
    "    logl_new = 0.0\n",
    "    newtondir = zeros(d)\n",
    "    \n",
    "    # obs matrix = diagonal(D_diag) - m11'\n",
    "    D_diag = zeros(d)\n",
    "    m = 0.0\n",
    "    # check for pd remedy \n",
    "    m_bound = 0.0 \n",
    "    \n",
    "    # iteration count\n",
    "    iteration = 0\n",
    "    \n",
    "    # Newton loop\n",
    "    for iter in 1:maxiters\n",
    "        # evaluate gradient (score)\n",
    "        gradient = dirmult_score(X_nz, α_old)\n",
    "        \n",
    "        # compute inverse obs matrix\n",
    "        m, D_diag = dirmult_obs(X_nz, α_old)\n",
    "        \n",
    "        # remedy for making sure obs matrix is pd\n",
    "        sum_dinv = sum(1 ./ D_diag)\n",
    "        \n",
    "        if m >= 1 / sum_dinv\n",
    "            m = 0.95 * 1 / sum_dinv\n",
    "        end\n",
    "        \n",
    "        # compute Newton's direction based on Sherman-Morrison formula\n",
    "        newtondir .= (1 ./ D_diag) .* gradient + 1 / (1 / m - sum_dinv) *\n",
    "            ((1 ./ D_diag).^2) .* gradient\n",
    "        \n",
    "        # line search loop\n",
    "        for lsiter in 1:10\n",
    "            # step halving\n",
    "            step = 0.5^(lsiter - 1)\n",
    "            \n",
    "            # get new α\n",
    "            α_new .= α_old + step .* newtondir\n",
    "            logl_new = sum(dirmult_logpdf(X_nz, α_new))\n",
    "            \n",
    "            # check for getting higher loglikelihood fcn\n",
    "            if logl_new > logl_old\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # check convergence criterion\n",
    "        if abs(logl_new - logl_old) < tolfun * (abs(logl_old) + 1)\n",
    "            # update before break the loop\n",
    "            α_old = copy(α_new)\n",
    "            logl_old = logl_new\n",
    "            iteration += 1\n",
    "            break;\n",
    "        end\n",
    "        \n",
    "        # update \n",
    "        α_old = copy(α_new)\n",
    "        logl_old = logl_new\n",
    "        iteration += 1\n",
    "        \n",
    "    end\n",
    "    \n",
    "    # add zeros back to α vector\n",
    "    α = zeros(α_d)\n",
    "    α[row_ind] = α_old[1:d]\n",
    "\n",
    "    \n",
    "    \n",
    "    # compute logl, gradient, Hessian from final iterate\n",
    "    logl = sum(dirmult_logpdf(X_nz, α))\n",
    "    gradient = dirmult_score(X_nz, α)\n",
    "    m, D_diag = dirmult_obs(X_nz, α)\n",
    "    hessian = (Diagonal(vec(D_diag)) - m * ones(α_d, α_d)) * (-1)\n",
    "\n",
    "    # output\n",
    "    return α, logl, gradient, hessian, m, D_diag, iteration, α_d\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "α0 = vec(dirmult_α0(train_x'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64-element Array{Float64,1}:\n",
       " 0.0        \n",
       " 0.0805772  \n",
       " 0.888965   \n",
       " 2.08629    \n",
       " 2.00262    \n",
       " 0.759017   \n",
       " 0.145656   \n",
       " 0.0150775  \n",
       " 0.000387975\n",
       " 0.299797   \n",
       " 1.73341    \n",
       " 2.12608    \n",
       " 1.8558     \n",
       " ⋮          \n",
       " 1.64636    \n",
       " 1.39109    \n",
       " 0.456776   \n",
       " 0.0271295  \n",
       " 0.000129274\n",
       " 0.0651678  \n",
       " 0.913522   \n",
       " 2.0955     \n",
       " 1.92583    \n",
       " 0.938191   \n",
       " 0.230338   \n",
       " 0.0276724  "
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirmult_newton(train_x', α0 = α0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0276724, 0.0, 0.0], -525538.0707315523, [NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN  …  NaN, NaN, NaN, NaN, NaN, NaN, NaN, -154416.0, 0.0, 0.0], [NaN 4.99845e6 … 4.99845e6 4.99845e6; 4.99845e6 NaN … 4.99845e6 4.99845e6; … ; 4.99845e6 4.99845e6 … 4.99845e6 4.99845e6; 4.99845e6 4.99845e6 … 4.99845e6 4.99845e6], 4.998449589767135e6, [NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN  …  NaN, NaN, NaN, NaN, NaN, NaN, NaN, 2.70494e5, 0.0, 0.0], 43, 64)"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = dirmult_newton(train_x', α0 = α0)\n",
    "obs = Diagonal(vec(D_diag)) - m * ones(d, d)\n",
    "det(obs)\n",
    "result[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Read in `optdigits.tra`, the training set of 3823 handwritten digits. Find the MLE for the subset of digit 0, digit 1, ..., and digit 9 separately using your function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in train data\n",
    "path1_tra = \"http://hua-zhou.github.io/teaching/biostatm280\"\n",
    "path2_tra = \"-2018spring/hw/hw4/optdigits.tra\"\n",
    "\n",
    "train = readdlm(download(path1_tra * path2_tra), ',')\n",
    "\n",
    "# separte last column which store the digit \n",
    "train_x = round.(Int64, train[:, 1:64])\n",
    "train_digit = round.(Int64, train[:, 65]);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×6 SubArray{Float64,2,Array{Float64,2},Tuple{UnitRange{Int64},UnitRange{Int64}},false}:\n",
       " 14.0   7.0   1.0   0.0  0.0  0.0\n",
       " 16.0  15.0   3.0   0.0  0.0  0.0\n",
       " 14.0   0.0   0.0   0.0  0.0  7.0\n",
       "  1.0  15.0   2.0   0.0  0.0  4.0\n",
       " 12.0  14.0   7.0   0.0  0.0  6.0\n",
       " 16.0  16.0  16.0  16.0  6.0  2.0\n",
       " 13.0   5.0   0.0   0.0  0.0  5.0\n",
       " 13.0   8.0   0.0   0.0  0.0  5.0\n",
       " 12.0   5.0   0.0   0.0  0.0  0.0\n",
       " 15.0  11.0   6.0   0.0  0.0  8.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " 0.0\n",
       " 0.0\n",
       " 7.0\n",
       " 4.0\n",
       " 6.0\n",
       " 2.0\n",
       " 5.0\n",
       " 5.0\n",
       " 0.0\n",
       " 8.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2×6 Array{Float64,2}:\n",
       " 13.0  5.0  0.0  0.0  0.0  5.0\n",
       " 13.0  8.0  0.0  0.0  0.0  5.0"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=view(train, 1:10, 60:65)\n",
    "display(a)\n",
    "display(a[:, 6])\n",
    "a[a[:, 6] .== 5.0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376×65 Array{Int64,2}:\n",
       " 0  1   6  15  12   1   0  0  0   7  16  …  0  0  0   6  14   7   1   0  0  0\n",
       " 0  0  10  16   6   0   0  0  0   7  16     0  0  0  10  16  15   3   0  0  0\n",
       " 0  0   8  15  16  13   0  0  0   1  11     0  0  0   9  14   0   0   0  0  7\n",
       " 0  0   0   3  11  16   0  0  0   0   5     0  0  0   0   1  15   2   0  0  4\n",
       " 0  0   5  14   4   0   0  0  0   0  13     0  0  0   4  12  14   7   0  0  6\n",
       " 0  0  11  16  10   1   0  0  0   4  16  …  3  0  0  10  16  16  16  16  6  2\n",
       " 0  0   1  11  13  11   7  0  0   0   9     0  0  0   1  13   5   0   0  0  5\n",
       " 0  0   8  10   8   7   2  0  0   1  15     0  0  0   4  13   8   0   0  0  5\n",
       " 0  0  15   2  14  13   2  0  0   0  16     0  0  0  10  12   5   0   0  0  0\n",
       " 0  0   3  13  13   2   0  0  0   6  16     0  0  0   3  15  11   6   0  0  8\n",
       " 0  0   6  14  14  16  16  8  0   0   7  …  0  0  0  10  12   0   0   0  0  7\n",
       " 0  0   0   3  16  11   1  0  0   0   0     0  0  0   0   2  14  14   1  0  1\n",
       " 0  0   0   4  13  16  16  3  0   0   8     0  0  0   0   5  15   4   0  0  9\n",
       " ⋮                  ⋮                 ⋮  ⋱  ⋮                 ⋮              \n",
       " 0  0   5  13  14   1   0  0  0   0  14     0  0  0   4  14  15   1   0  0  9\n",
       " 0  0   1  13  10   0   0  0  0   0  10  …  0  0  0   0   8  15   9   0  0  6\n",
       " 0  1   7  14  16  11   0  0  0  11  16     0  0  0  10  15   9   1   0  0  3\n",
       " 0  1   9  14   7   2   0  0  0   8  16     0  0  0  12  14   8   0   0  0  9\n",
       " 0  0   0   0  13   6   0  0  0   0   1     0  0  0   0   1  14   5   0  0  4\n",
       " 0  0   5  12  13   1   0  0  0   3  15     0  0  0   6  14  12   1   0  0  8\n",
       " 0  0   9  16  16   8   0  0  0   2  16  …  0  0  0  10  15   6   0   0  0  5\n",
       " 0  0  13  10   2   8   0  0  0   2  16     0  0  0  11  13   2   0   0  0  8\n",
       " 0  1  12  16  10   1   0  0  0  11  15     0  0  0   9   8   1   0   0  0  9\n",
       " 0  0   0   0  11   9   0  0  0   0   0     0  0  0   0   0  10   4   0  0  4\n",
       " 0  0   4  15  16   6   0  0  0   0  14     0  0  0   5  12  14   6   0  0  0\n",
       " 0  0   4  13  12   1   0  0  0   2  15  …  0  0  0   8  13   3   0   0  0  8"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_0 = X[X[:, 65] .== 0, 1:64]\n",
    "X_1 = X[X[:, 65] .== 1, 1:64]\n",
    "#dirmult_newton(X_0', α0 = α0)[1]\n",
    "#dirmult_newton(X_1', α0 = α0)[1]\n",
    "    # remove rows with all 0 entries\n",
    "    row_ind = find(sum(X_0, 2))\n",
    "    #col_ind = find(sum(X_0, 1))\n",
    "    #display(row_ind)\n",
    "    # get the new X removing all the 0 sum lines\n",
    "    X_nz = X[row_ind, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "α0 = [0.0, 0.00455717, 0.69269, 2.66443, 3.27205, 1.7059, 0.153425, 0.0, 0.0, 0.0645599, 1.41196, 3.72473, 4.38172, 2.73127, 0.268873, 0.0, 0.00987387, 0.4185, 2.38492, 4.33615, 4.40755, 2.41378, 0.16178, 0.0, 0.00303812, 0.723071, 2.98647, 4.33691, 4.33615, 2.21631, 0.135196, 0.0, 0.0, 0.481541, 2.20415, 3.56751, 4.28222, 2.14719, 0.119246, 0.0, 0.0, 0.11317, 1.3907, 3.11483, 4.20019, 2.28238, 0.164818, 0.0, 0.0, 0.040255, 1.32158, 3.2318, 4.27463, 2.81709, 0.48382, 0.0562051, 0.0, 0.0113929, 0.688133, 2.55202, 3.71334, 2.65531, 0.688893, 0.147349]\n",
      "(dirmult_newton(X_digit', α0=α0))[1] = [0.0, 0.0183493, 1.40104, 6.32803, 9.02642, 3.82976, 0.331836, 0.0, 0.0, 0.134288, 3.03424, 10.0597, 12.0435, 6.66139, 0.449188, 0.0, 0.0148685, 0.626685, 5.68392, 11.9302, 12.0181, 5.54624, 0.260292, 0.0, 0.0146434, 1.40655, 7.7252, 11.9444, 11.9094, 4.99621, 0.221519, 0.0, 0.0, 0.916353, 4.93265, 9.48779, 11.8298, 4.82205, 0.189065, 0.0, 0.0, 0.249257, 2.44194, 7.56206, 11.6648, 5.22852, 0.244245, 0.0, 0.0, 0.084297, 2.39944, 8.12133, 11.7923, 6.86782, 0.72481, 0.071186, 0.0, 0.0406592, 1.40384, 6.07178, 10.4093, 6.56163, 1.12001, -3.2362e7]\n"
     ]
    }
   ],
   "source": [
    "# convert the training data set to Int64\n",
    "X = round.(Int64, train);\n",
    "\n",
    "# initialize matrix to store \n",
    "α_all = zeros(64, 10) \n",
    "\n",
    "for digit in 1\n",
    "    # get the train_x subset for different digits\n",
    "    X_digit = X[X[:, 65] .== digit, 1:64]\n",
    "    α0 = vec(dirmult_α0(X_digit'))\n",
    "    @show α0 \n",
    "    # get \n",
    "    #α_all[:, (digit + 1)] .= \n",
    "    @show dirmult_newton(X_digit', α0 = α0)[1]\n",
    "    \n",
    "    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_dinv = 0.2979935880757474\n",
      "m = 3.1145035392971185\n",
      "det(obs) = 6.614177955021555e138\n",
      "newtondir = [0.00217632, -0.145322, -0.32907, 0.343614, -0.253604, -0.00244879, -0.00214138, -0.294864, 0.0583917, 0.549382, -0.212786, -0.098471, -0.00398023, -0.244769, -0.227718, 0.570924, 0.605744, -0.381936, -0.0617552, 0.00171308, -0.195795, 0.0231154, 0.55724, 0.590751, -0.396347, -0.0473989, -0.123812, -0.4181, 0.0145702, 0.555543, -0.388321, -0.0467336, 0.00131763, -0.731571, -0.392634, 0.514463, -0.359141, -0.0820093, -0.000703281, -0.615894, -0.233743, 0.592259, -0.255434, -0.293289, -0.0394985, 0.00466516, -0.137428, -0.297273, 0.486891, -0.137212, -0.360346, -0.333323]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52-element Array{Float64,1}:\n",
       "  0.0067335 \n",
       "  0.547368  \n",
       "  2.33536   \n",
       "  3.61566   \n",
       "  1.4523    \n",
       "  0.150976  \n",
       "  0.0624186 \n",
       "  1.1171    \n",
       "  3.78312   \n",
       "  4.9311    \n",
       "  2.51848   \n",
       "  0.170402  \n",
       "  0.00589365\n",
       "  ⋮         \n",
       "  2.99805   \n",
       "  4.86689   \n",
       "  2.56166   \n",
       "  0.190531  \n",
       "  0.0167067 \n",
       "  0.0160581 \n",
       "  0.550705  \n",
       "  2.25474   \n",
       "  4.20023   \n",
       "  2.5181    \n",
       "  0.328547  \n",
       " -0.185974  "
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = round.(Int64, train);\n",
    "X = (X1[X1[:, 65] .== 1, 1:64])'\n",
    "\n",
    "    # store original size of α\n",
    "    α_d = size(X, 1)\n",
    "    \n",
    "    # remove rows with all 0 entries\n",
    "    row_ind = find(sum(X, 2))\n",
    "    col_ind = find(sum(X, 1))\n",
    "    \n",
    "    # get the new X removing all the 0 sum lines\n",
    "    X_nz = X[row_ind, :]\n",
    "    \n",
    "    # store the number of parameters \n",
    "    d = size(X_nz, 1)\n",
    "    \n",
    "    # initialization\n",
    "    α0 = dirmult_α0(X)\n",
    "    α_old = α0[row_ind]\n",
    "    logl_old = sum(dirmult_logpdf(X_nz, α_old))\n",
    "    \n",
    "    # pre-allocation \n",
    "    α_new = zeros(d)\n",
    "    gradient = zeros(d)\n",
    "    \n",
    "    logl_new = 0.0\n",
    "    newtondir = zeros(d)\n",
    "    \n",
    "    # obs matrix = diagonal(D_diag) - m11'\n",
    "    D_diag = zeros(d)\n",
    "    m = 0.0\n",
    "    # check for pd remedy \n",
    "    m_bound = 0.0 \n",
    "    \n",
    "    # iteration count\n",
    "    iteration = 0\n",
    "\n",
    "\n",
    "\n",
    "for iter in 1\n",
    "        # evaluate gradient (score)\n",
    "        gradient = dirmult_score(X_nz, α_old)\n",
    "    \n",
    "        \n",
    "        # compute inverse obs matrix\n",
    "        m, D_diag = dirmult_obs(X_nz, α_old)\n",
    "        \n",
    "        # remedy for making sure obs matrix is pd\n",
    "        sum_dinv = sum(1 ./ D_diag)\n",
    "        @show sum_dinv\n",
    "    \n",
    "        if m >= 1 / sum_dinv\n",
    "            m = 0.95 * 1 / sum_dinv\n",
    "        end\n",
    "        @show m\n",
    "        \n",
    "        # check pd\n",
    "        obs = Diagonal(vec(D_diag)) - m * ones(d, d)\n",
    "        @show det(obs)\n",
    "        \n",
    "    \n",
    "        # compute Newton's direction based on Sherman-Morrison formula\n",
    "        newtondir .= (1 ./ D_diag) .* gradient + 1 / (1 / m - sum_dinv) *\n",
    "            ((1 ./ D_diag).^2) .* gradient\n",
    "        @show newtondir\n",
    "        \n",
    "        # line search loop\n",
    "        for lsiter in 1:2\n",
    "            # step halving\n",
    "            step = 0.5^(lsiter - 1)\n",
    "            \n",
    "            # get new α\n",
    "            α_new .= α_old + step .* newtondir\n",
    "            logl_new = sum(dirmult_logpdf(X_nz, α_new))\n",
    "            \n",
    "            # check for getting higher loglikelihood fcn\n",
    "            if logl_new > logl_old\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    \n",
    "        # check convergence criterion\n",
    "        if abs(logl_new - logl_old) < 1e-6 * (abs(logl_old) + 1)\n",
    "            # update before break the loop\n",
    "            α_old = copy(α_new)\n",
    "            logl_old = logl_new\n",
    "            iteration += 1\n",
    "            break;\n",
    "        end\n",
    "        \n",
    "        # update \n",
    "        α_old = copy(α_new)\n",
    "        logl_old = logl_new\n",
    "        iteration += 1\n",
    "        \n",
    "    end\n",
    "\n",
    "α_old \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mMethodError: no method matching find(::Bool, ::Array{Int64,1})\u001b[0m\nClosest candidates are:\n  find(\u001b[91m::Compat.OccursIn\u001b[39m, ::Any) at C:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\Users\\Dell\\AppData\\Local\\JuliaPro-0.6.2.2\\pkgs-0.6.2.2\\v0.6\\Compat\\src\\Compat.jl:1538\n  find(\u001b[91m::Function\u001b[39m, ::Any) at array.jl:1518\n  find(::Number) at array.jl:1569\n  ...\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mMethodError: no method matching find(::Bool, ::Array{Int64,1})\u001b[0m\nClosest candidates are:\n  find(\u001b[91m::Compat.OccursIn\u001b[39m, ::Any) at C:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\Users\\Dell\\AppData\\Local\\JuliaPro-0.6.2.2\\pkgs-0.6.2.2\\v0.6\\Compat\\src\\Compat.jl:1538\n  find(\u001b[91m::Function\u001b[39m, ::Any) at array.jl:1518\n  find(::Number) at array.jl:1569\n  ...\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\loading.jl:522\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "x = [-1, 1,4,3,4,4];\n",
    "f(x) = x == 4\n",
    "find(f(4), x)\n",
    "length(x[x .< 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
