{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M280 Homework4\n",
    "* Author: Shuang Gao\n",
    "* Date: 2018/06/01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "For a multivariate count vector $\\mathbf{x}=(x_1,\\ldots,x_d)$ with batch size $|\\mathbf{x}|=\\sum_{j=1}^d x_j$, show that the probability mass function for Dirichlet-multinomial distribution is\n",
    "$$\n",
    "    f(\\mathbf{x} \\mid \\alpha) \n",
    "\t= \\int_{\\Delta_d} \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\pi(\\mathbf{p}) \\, d \\mathbf{p}  \n",
    "    = \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}\n",
    "$$\n",
    "where $\\Delta_d$ is the unit simplex in $d$ dimensions and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\mathbf{x} \\mid \\alpha) \n",
    "&= \\int_{\\Delta_d} \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\pi(\\mathbf{p}) \\, d \\mathbf{p} \\\\\n",
    "&= \\int_{\\Delta_d} \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1}\\,d \\mathbf{p}\\\\\n",
    "&= \\binom{|\\mathbf{x}|}{\\mathbf{x}} \n",
    "\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\int_{\\Delta_d} \\prod_{j=1}^d p_j^{x_j+\\alpha_j-1}\\, d\\mathbf{p}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Because $\\mathbf{p} = (p_1,\\ldots, p_d)$ follow a Dirichlet distribution with parameter vector $\\alpha = (\\alpha_1,\\ldots, \\alpha_d)$, $\\alpha_j>0$, and density\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\pi(\\mathbf{p}) =  \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1},\n",
    "\\end{eqnarray*} \n",
    "$$\n",
    "where $|\\alpha|=\\sum_{j=1}^d \\alpha_j$.\n",
    "We can get,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\int \\pi(\\mathbf{p})\\,d\\mathbf{p} \n",
    "&= \\int \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\,d\\mathbf{p}=1\\\\\n",
    "& \\therefore \\int \\prod_{j=1}^d p_j^{\\alpha_j-1}\\,d\\mathbf{p}=\\frac{\\prod_{j=1}^d\\Gamma{(\\alpha_j})}{\\Gamma{(|\\alpha|)}}\\hspace{12mm}(1)\n",
    "\\end{align}\n",
    "$$\n",
    "In order to get the derivation of $f(\\mathbf{x} \\mid \\alpha) $, Plug $\\alpha_j=\\alpha_j+x_j$ into equation (1).\n",
    "Then we have,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\int \\prod_{j=1}^d p_j^{\\alpha_j+x_j-1}\\,d\\mathbf{p}=\\frac{\\prod_{j=1}^d\\Gamma{(\\alpha_j}+x_j)}{\\Gamma{(|\\alpha|+|\\mathbf{x}|)}}\n",
    "\\end{align}\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\mathbf{x} \\mid \\alpha) \n",
    "&= \\binom{|\\mathbf{x}|}{\\mathbf{x}} \n",
    "\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\prod_{j=1}^d\\Gamma{(\\alpha_j}+x_j)}{\\Gamma{(|\\alpha|+|\\mathbf{x}|)}}\\\\\n",
    "&=  \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Given independent data points $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$, show that the log-likelihood is\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)]\n",
    "$$\n",
    "Is the log-likelihood a concave function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\because \\mathbf{x_1},...,\\mathbf{x_n} \\text{ are independent}\\\\ \n",
    "& \\therefore f(\\mathbf{x_1},...,\\mathbf{x_n} \\mid \\alpha) \n",
    "= f(\\mathbf{x_1}\\mid \\alpha)...f(\\mathbf{x_n\\mid \\alpha})\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\therefore f(\\mathbf{x_1},...,\\mathbf{x_n} \\mid \\alpha) \n",
    "= \\prod_{i=1}^n \\binom{|\\mathbf{x_i}|}{\\mathbf{x_i}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_{ij})}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x_i}|)}\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\therefore L(\\alpha) \n",
    "&= \\ln (\\prod_{i=1}^n \\binom{|\\mathbf{x_i}|}{\\mathbf{x_i}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_{ij})}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x_i}|)})\\\\\n",
    "&= \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* The function is not concave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Write Julia function to compute the log-density of the Dirichlet-multinomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log density of the Dirichlet-multinomial distribution for one data point $\\mathbf{x}$ is shown as follow:\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\alpha) \n",
    "&= \\ln \\binom{|\\mathbf{x}|}{\\mathbf{x}} +  \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{j}) - \\ln \\Gamma(\\alpha_j)] - [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}|) - \\ln \\Gamma(|\\alpha|)]\\\\\n",
    "&=  \\ln |\\mathbf{x}|! - \\ln(x_1!)-...-\\ln(x_{64}!)+ \\sum_{j=1}^{64} [\\ln \\Gamma(\\alpha_j + x_{j}) - \\ln \\Gamma(\\alpha_j)] - [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}|) - \\ln \\Gamma(|\\alpha|)]\\\\\n",
    "&=  \\ln |\\mathbf{x}|! - [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}|) - \\ln \\Gamma(|\\alpha|)]+ \\sum_{j=1}^{64} [\\ln \\Gamma(\\alpha_j + x_{j}) - \\ln \\Gamma(\\alpha_j)-\\ln(x_j!)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_logpdf(x::Vector, α::Vector)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at data point `x`.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(x::Vector, α::Vector)\n",
    "    # store the sum of x and α\n",
    "    sumx = sum(x)\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    # store length of x\n",
    "    lengthx = size(x, 1) \n",
    "    \n",
    "    # part without summation\n",
    "    logden = lfact(sumx) - lgamma(sumα + sumx) + lgamma(sumα)\n",
    "    \n",
    "    # part within summation\n",
    "    for j in lengthx\n",
    "       logden += lgamma(α[j] + x[j]) - lgamma(α[j]) - lfact(x[j])  \n",
    "    end\n",
    "    \n",
    "    # return log density\n",
    "    return logden\n",
    "end\n",
    "\n",
    "function dirmult_logpdf!(r::Vector, X::Matrix, α::Vector)\n",
    "    for j in 1:size(X, 2)\n",
    "        r[j] = dirmult_logpdf(X[:, j], α)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at each data point in `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::Matrix, α::Vector)\n",
    "    r = zeros(size(X, 2))\n",
    "    dirmult_logpdf!(r, X, α)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Read in `optdigits.tra`, the training set of 3823 handwritten digits. Each row contains the 64 counts of a digit and the last element (65th element) indicates what digit it is. For grading purpose, evaluate the total log-likelihood of this data at parameter values $\\alpha=(1,\\ldots,1)$ using your function in Q3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in train data\n",
    "path1_tra = \"http://hua-zhou.github.io/teaching/biostatm280\"\n",
    "path2_tra = \"-2018spring/hw/hw4/optdigits.tra\"\n",
    "\n",
    "train = readdlm(download(path1_tra * path2_tra), ',')\n",
    "\n",
    "# separte last column which store the digit \n",
    "train_x = round.(Int64, train[:, 1:64])\n",
    "train_digit = round.(Int64, train[:, 65]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-638817.993292528"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store size of train data\n",
    "length_x = size(train_x, 2)\n",
    "length_obs = size(train_x, 1)\n",
    "\n",
    "# input α vector\n",
    "α = round.(Int64, ones(length_x))\n",
    "\n",
    "sum(dirmult_logpdf(train_x', α))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The total log-likelihood of this data at parameter values $\\alpha=(1,\\ldots,1)$ is around -638818."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question5 \n",
    "Derive the score function $\\nabla L(\\alpha)$, observed information matrix $-d^2L(\\alpha)$, and Fisher information matrix $\\mathbf{E}[-d^2L(\\alpha)]$ for the Dirichlet-multinomial distribution.\n",
    "\n",
    "Comment on why Fisher scoring method is inefficient for computing MLE in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "* **score function**\n",
    "\n",
    "From part 2, we have \n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)]\n",
    "$$\n",
    "By taking first derivative of $L(\\alpha)$ respect to each element in vector $\\alpha$, we get the socre function to be as follow,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla L(\\alpha) \n",
    "&= \\sum_{i = 1}^n (\\Psi(\\alpha_j+x_{ij})-\\Psi(\\alpha_j)) - \\sum_{i=1}^n(\\Psi(|\\alpha|+|x_i|)-\\Psi(|\\alpha|))\\\\\n",
    "&= \\sum_{i = 1}^n (\\Psi(\\alpha_j+x_{ij})-\\Psi(\\alpha_j)) - \\sum_{i=1}^n\\Psi(|\\alpha|+|x_i|)+n\\Psi(|\\alpha|)\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\Psi(x)= \\frac{d}{dx}log\\Gamma(x)$ is the digamma function.\n",
    "* **observed information matrix**\n",
    "\n",
    "$$\n",
    "-d^2L(\\alpha)=-\\frac{d\\nabla L(\\alpha)}{d\\alpha}\n",
    "=\\sum_{i = 1}^n[\\Psi'(\\alpha_j)-\\Psi'(\\alpha_j+x_{ij})]-\\sum_{i = 1}^n[\\Psi'(|\\alpha|)-\\Psi'(|\\alpha|+|x_i|)]\n",
    "$$\n",
    "(1) diagonal matrix part, $D$\n",
    "\n",
    "When taking the second derivative respect to $\\alpha$, the terms with only $\\alpha_j$ will pretain if both of the derivatives taken respect to $\\alpha_j$. That is $\\Psi'(\\alpha_j)$ and $\\Psi'(\\alpha_j+x_{ij})$ terms will only appear on the diaganol of observed information matrix. Therefore, let's define a diagonal matrix $D$, which has the $j^{th}$diagonal entry shown as\n",
    "$\\sum_{i = 1}^n[\\Psi'(\\alpha_j)-\\Psi'(\\alpha_j+x_{ij})]$.\n",
    "From recurrence relation of polygamma function $\\Psi^{(m)}(z+1)=\\Psi^{(m)}(z)+\\frac{(-1)^mm!}{z^{m+1}}$, the trigamma function$\\Psi'(z+1)=\\Psi^{(1)}(z+1)=\\Psi^{(1)}(z)+\\frac{(-1)^mm!}{z^{m+1}}$.\n",
    "\n",
    "Then,\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\Psi^{(1)}(\\alpha_j+1)=\\Psi^{(1)}(\\alpha_j)-\\frac{1}{\\alpha_j^2}\\\\\n",
    "&\\Psi^{(1)}(\\alpha_j+2)=\\Psi^{(1)}(\\alpha_j+1)-\\frac{1}{\\alpha_j^2+1}\\\\\n",
    "&.\\\\\n",
    "&.\\\\\n",
    "&.\\\\\n",
    "&\\Psi^{(1)}(\\alpha_j+x_{ij})=\\Psi^{(1)}(\\alpha_j+x_{ij}-1)-\\frac{1}{(\\alpha_j+x_{ij}-1)^2}\n",
    "\\end{align}\n",
    "$$\n",
    "By suming all the above equations, we get\n",
    "$\\Psi'(\\alpha_j)-\\Psi'(\\alpha_j+x_{ij})=\\sum_{p=0}^{x_{ij}-1} \\frac{1}{(\\alpha_j+p)^2}$\n",
    "Therefore, the $j_{th}$ diagonal position in matrix $D$, \n",
    "$$\\sum_{i = 1}^n[\\Psi'(\\alpha_j)-\\Psi'(\\alpha_j+x_{ij})]=\\sum_{i = 1}^n\\sum_{p=0}^{x_{ij}-1} \\frac{1}{(\\alpha_j+p)^2}$$\n",
    "(2) other part, $m$\n",
    "\n",
    "Similarly, the terms left in the observated information matrix is \n",
    "$$\n",
    "m= \\sum_{i = 1}^n[\\Psi'(|\\alpha|)-\\Psi'(|\\alpha|+|x_i|)]\n",
    "= \\sum_{i = 1}^n\\sum_{p=0}^{|x_{i}|-1} \\frac{1}{(|\\alpha|+p)^2}\n",
    ",$$ which is a constant over the whole matrix.\n",
    "\n",
    "Therefore, the observed info matrix can be written as \n",
    "$\\mathbf{D}-m\\mathbf{1}\\mathbf{1}'$, where $\\mathbf{D}$ and $m$ are defined previously.\n",
    "\n",
    "* **Fisher information matrix $\\mathbf{E}[-d^2L(\\alpha)]$ **\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{E}[-d^2L(\\alpha)]\n",
    "&=\\mathbf{E}(\\mathbf{D}-m\\mathbf{1}\\mathbf{1}')\\\\\n",
    "&=\\mathbf{E}(\\mathbf{D})-m\\mathbf{1}\\mathbf{1}'\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "For calculating the expected value, we need to integrate over $\\alpha$, which does not have analytical solution. Therefore, Fisher scoring method is insufficient for computing MLE in this example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What structure does the observed information matrix possess that can facilitate the evaluation of the Newton direction? Is the observed information matrix always positive definite? What remedy can we take if it fails to be positive definite? (Hint: HW1 Q6.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Solution**\n",
    "\n",
    "1.special structure\n",
    "\n",
    "From part 5, we can see the observed information matrix $[-d^2L(\\alpha)]=\\mathbf{D}-m\\mathbf{1}\\mathbf{1}'$, where $D$ is a diagonal matrix with $j^{th}$ diagonal entry, $\\sum_{i = 1}^n\\sum_{p=0}^{x_{ij}-1} \\frac{1}{(\\alpha_j+p)^2}$ and $m=\\sum_{i = 1}^n\\sum_{p=0}^{|x_{i}|-1} \\frac{1}{(|\\alpha|+p)^2}$ is a constant. Therefore the observed information matrix is composed by a diagonal matrix and a rank one matrix. \n",
    "\n",
    "2.not always positive difinite \n",
    "$$\n",
    "\\begin{align}\n",
    "det[-d^2L(\\alpha)] \n",
    "&= det[\\mathbf{D}-m\\mathbf{1}\\mathbf{1}'] \\\\\n",
    "&= det[\\mathbf{D}(\\mathbf{I}-\\mathbf{D}^{-1}m\\mathbf{1}\\mathbf{1}')] \\\\\n",
    "&= det(\\mathbf{D})det(\\mathbf{I}-m\\mathbf{D}^{-1}\\mathbf{1}\\mathbf{1}')\\\\\n",
    "&= (\\prod_{j}d_j)(1-\\sum_j\\frac{m}{d_j})\n",
    "\\end{align}\n",
    "$$\n",
    "In order to make the matrix positive definite, the determant of the matrix should be positive. From the above equation, that is $1-\\sum_j\\frac{m}{d_j}>0$ should be satisfied. However, it is not always the case. Therefore, the matrix is not necessarily positive definite.\n",
    "\n",
    "3.remedy for pd\n",
    "\n",
    "In order to make the matrix positive definite, $1-\\sum_j\\frac{m}{d_j}>0$ should be satisfied. That is, $m<\\frac{1}{\\sum_j(dj^{-1})}$\n",
    "\n",
    "Therefore, we can take the remedy according to the following rules,\n",
    "$$\n",
    "m_{new} = \n",
    "\\begin{cases}\n",
    "m & \\text{if } m<\\frac{1}{\\sum_j(dj^{-1})}\\\\\n",
    "0.9 \\frac{1}{\\sum_j(dj^{-1})}& \\text{o.w.}  \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "4.facillitate the evaluation of the Newton direction.\n",
    "\n",
    "\n",
    "With the Sherman-Morrison forula, $$\n",
    "\t(\\mathbf{A} + \\mathbf{u} \\mathbf{u}^T)^{-1} = \\mathbf{A}^{-1} - \\frac{1}{1 + \\mathbf{u}^T \\mathbf{A}^{-1} \\mathbf{u}} \\mathbf{A}^{-1} \\mathbf{u} \\mathbf{u}^T \\mathbf{A}^{-1},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "[-d^2L(\\alpha)]^{-1}\n",
    "&= [\\mathbf{D}-m\\mathbf{1}\\mathbf{1}']^{-1}\\\\\n",
    "&= \\mathbf{D}^{-1}+\\frac{m}{1+m\\mathbf{1}'\\mathbf{D}^{-1}\\mathbf{1}}\\mathbf{D}^{-1}\\mathbf{1}\\mathbf{1}'\\mathbf{D}^{-1}\\\\\n",
    "&= \\mathbf{D}^{-1}+\\frac{1}{m^{-1}+\\mathbf{1}'\\mathbf{D}^{-1}\\mathbf{1}}\\mathbf{D}^{-1}\\mathbf{1}\\mathbf{1}'\\mathbf{D}^{-1}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In this way, the inverse of observed info matrix can be decomposed to calculate the inverse of a diagonal matrix $\\mathbf{D}$. In this way, the evaluation of the Newton direction can be facillitated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Discuss how to choose a good starting point. Implement this as the default starting value in your function below. (Hint: Method of moment estimator may furnish a good starting point.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Solution**\n",
    "\n",
    "From the Dirichlet distruibution, we have the first and second theoritical moments shown as below,\n",
    "$$\n",
    "\\mathbf{E}(p_k)=\\frac{\\alpha_k}{|\\alpha|}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{E}(p_k^2)=\\frac{\\alpha_k(\\alpha_k+1)}{|\\alpha|(1+|\\alpha|)}\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\sum_k\\frac{\\mathbf{E}(p_j^2)}{\\mathbf{E}(p_j)} \n",
    "= \\sum_k\\frac{\\frac{\\alpha_k(\\alpha_k+1)}{|\\alpha|(1+|\\alpha|)}}{\\frac{\\alpha_k}{|\\alpha|}}\n",
    "= \\sum_k\\frac{\\alpha_k+1}{1+|\\alpha|}\n",
    "= \\frac{|\\alpha|+d}{|\\alpha|+1}\n",
    "$$\n",
    "From the n observations, we have the first and second sample moments shown as below,\n",
    "$$\n",
    "\\mathbf{E}(p_k)=\\frac{1}{n}\\sum_{i=1}^n\\frac{x_{ik}}{|x_i|}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{E}(p_k^2)=\\frac{1}{n}\\sum_{i=1}^n(\\frac{x_{ik}}{|x_i|})^2\n",
    "$$\n",
    "then let constant c defined as follow,\n",
    "$$\n",
    "c=\\sum_k\\frac{\\mathbf{E}(p_j^2)}{\\mathbf{E}(p_j)} \n",
    "=\\sum_k\\frac{\\frac{1}{n}\\sum_{i=1}^n(\\frac{x_{ik}}{|x_i|})^2}{\\frac{1}{n}\\sum_{i=1}^n\\frac{x_{ik}}{|x_i|}}\n",
    "=\\sum_k\\frac{\\sum_{i=1}^n(\\frac{x_{ik}}{|x_i|})^2}{\\sum_{i=1}^n\\frac{x_{ik}}{|x_i|}}\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "\\frac{|\\alpha|+d}{|\\alpha|+1}=c\\\\\n",
    "(c+1)|\\alpha|=d-c\\\\\n",
    "|\\alpha|=\\frac{d-c}{c-1}\n",
    "$$\n",
    "In order to get each elements in $\\alpha$, \n",
    "$$\n",
    "\\alpha_k = |\\alpha|\\mathbf{E}(p_k)=\\frac{d-c}{c-1}\\frac{\\sum_i x_{ik}}{\\sum_i|\\mathbf{x_i}|}\n",
    "$$\n",
    "where $c=\\sum_k\\frac{\\sum_{i=1}^n(\\frac{x_{ik}}{|x_i|})^2}{\\sum_{i=1}^n\\frac{x_{ik}}{|x_i|}}$\n",
    "\n",
    "In this way, we can use the given data $x_{ik}$ where $i$ from 1 to number of all data points and $k$ is from 1 to $d=64$ to calculate the initial $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From part 2, we have \n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)]\n",
    "$$\n",
    "By taking first derivative of $L(\\alpha)$ respect to each element in vector $\\alpha$, we get the socre function to be as follow,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla L(\\alpha) \n",
    "&= \\sum_{i = 1}^n (\\Psi(\\alpha_j+x_{ij})-\\Psi(\\alpha_j)) - \\sum_{i=1}^n\\Psi(|\\alpha|+|x_i|)+\\Psi(|\\alpha|)\\\\\n",
    "&= \\sum_{i = 1}^n (\\Psi(\\alpha_j+x_{ij})-\\Psi(\\alpha_j)) - \\sum_{i=1}^n\\Psi(|\\alpha|+|x_i|)+n\\Psi(|\\alpha|)\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\Psi(x)= \\frac{d}{dx}log\\Gamma(x)$ is the digamma function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using the Newton's method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_score (generic function with 1 method)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to calculate score function of log-likelihood\n",
    "# take column of X as one data point\n",
    "function dirmult_score(X::Matrix, α::Vector, score::Vector)\n",
    "    # initialize output \n",
    "    score = zeros(length(α))\n",
    "    \n",
    "    # sum of alpha vector\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    # column sum of X\n",
    "    colsumX = sum(X, 1)\n",
    "    \n",
    "    # store the size of n and d\n",
    "    n = size(X, 2)\n",
    "    d = size(X, 1)\n",
    "    \n",
    "    # evalutate score function at input α\n",
    "    for j in 1:d\n",
    "        for i in 1:n\n",
    "            score[j] += digamma(α[j] + X[j, i]) - digamma(α[j]) - \n",
    "                digamma(sumα + colsumX[i]) + digamma(sumα)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # return vector score\n",
    "    return score \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64-element Array{Float64,1}:\n",
       " -6809.0  \n",
       " -5994.02 \n",
       "   399.53 \n",
       "  4465.77 \n",
       "  4255.45 \n",
       "  -239.022\n",
       " -4871.03 \n",
       " -6583.75 \n",
       " -6804.21 \n",
       " -3509.27 \n",
       "  3477.72 \n",
       "  4619.83 \n",
       "  3901.48 \n",
       "     ⋮    \n",
       "  3294.76 \n",
       "  2375.96 \n",
       " -2036.58 \n",
       " -6477.55 \n",
       " -6808.0  \n",
       " -6101.94 \n",
       "   525.922\n",
       "  4463.0  \n",
       "  3988.35 \n",
       "   655.835\n",
       " -3961.5  \n",
       " -6442.54 "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for score function \n",
    "score = zeros(α)\n",
    "dirmult_score(train_x', α, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pkg.add(\"SpecialFunctions.jl\")\n",
    "using SpecialFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to calculae the inverse of \n",
    "# observed information matrix with remedy\n",
    "function dirmult_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
