{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M280 Homework4\n",
    "* Author: Shuang Gao\n",
    "* Date: 2018/06/01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "For a multivariate count vector $\\mathbf{x}=(x_1,\\ldots,x_d)$ with batch size $|\\mathbf{x}|=\\sum_{j=1}^d x_j$, show that the probability mass function for Dirichlet-multinomial distribution is\n",
    "$$\n",
    "    f(\\mathbf{x} \\mid \\alpha) \n",
    "\t= \\int_{\\Delta_d} \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\pi(\\mathbf{p}) \\, d \\mathbf{p}  \n",
    "    = \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}\n",
    "$$\n",
    "where $\\Delta_d$ is the unit simplex in $d$ dimensions and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\mathbf{x} \\mid \\alpha) \n",
    "&= \\int_{\\Delta_d} \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j} \\pi(\\mathbf{p}) \\, d \\mathbf{p} \\\\\n",
    "&= \\int_{\\Delta_d} \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\prod_{j=1}^d p_j^{x_j}\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1}\\,d \\mathbf{p}\\\\\n",
    "&= \\binom{|\\mathbf{x}|}{\\mathbf{x}} \n",
    "\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\int_{\\Delta_d} \\prod_{j=1}^d p_j^{x_j+\\alpha_j-1}\\, d\\mathbf{p}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Because $\\mathbf{p} = (p_1,\\ldots, p_d)$ follow a Dirichlet distribution with parameter vector $\\alpha = (\\alpha_1,\\ldots, \\alpha_d)$, $\\alpha_j>0$, and density\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\pi(\\mathbf{p}) =  \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1},\n",
    "\\end{eqnarray*} \n",
    "$$\n",
    "where $|\\alpha|=\\sum_{j=1}^d \\alpha_j$.\n",
    "We can get,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\int \\pi(\\mathbf{p})\\,d\\mathbf{p} \n",
    "&= \\int \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\,d\\mathbf{p}=1\\\\\n",
    "& \\therefore \\int \\prod_{j=1}^d p_j^{\\alpha_j-1}\\,d\\mathbf{p}=\\frac{\\prod_{j=1}^d\\Gamma{(\\alpha_j})}{\\Gamma{(|\\alpha|)}}\\hspace{12mm}(1)\n",
    "\\end{align}\n",
    "$$\n",
    "In order to get the derivation of $f(\\mathbf{x} \\mid \\alpha) $, Plug $\\alpha_j=\\alpha_j+x_j$ into equation (1).\n",
    "Then we have,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\int \\prod_{j=1}^d p_j^{\\alpha_j+x_j-1}\\,d\\mathbf{p}=\\frac{\\prod_{j=1}^d\\Gamma{(\\alpha_j}+x_j)}{\\Gamma{(|\\alpha|+|\\mathbf{x}|)}}\n",
    "\\end{align}\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\mathbf{x} \\mid \\alpha) \n",
    "&= \\binom{|\\mathbf{x}|}{\\mathbf{x}} \n",
    "\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\prod_{j=1}^d\\Gamma{(\\alpha_j}+x_j)}{\\Gamma{(|\\alpha|+|\\mathbf{x}|)}}\\\\\n",
    "&=  \\binom{|\\mathbf{x}|}{\\mathbf{x}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_j)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x}|)}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Given independent data points $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$, show that the log-likelihood is\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)]\n",
    "$$\n",
    "Is the log-likelihood a concave function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\because \\mathbf{x_1},...,\\mathbf{x_n} \\text{ are independent}\\\\ \n",
    "& \\therefore f(\\mathbf{x_1},...,\\mathbf{x_n} \\mid \\alpha) \n",
    "= f(\\mathbf{x_1}\\mid \\alpha)...f(\\mathbf{x_n\\mid \\alpha})\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\therefore f(\\mathbf{x_1},...,\\mathbf{x_n} \\mid \\alpha) \n",
    "= \\prod_{i=1}^n \\binom{|\\mathbf{x_i}|}{\\mathbf{x_i}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_{ij})}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x_i}|)}\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\therefore L(\\alpha) \n",
    "&= \\ln (\\prod_{i=1}^n \\binom{|\\mathbf{x_i}|}{\\mathbf{x_i}} \\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j+x_{ij})}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\frac{\\Gamma(|\\alpha|)}{\\Gamma(|\\alpha|+|\\mathbf{x_i}|)})\\\\\n",
    "&= \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* The function is not concave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Write Julia function to compute the log-density of the Dirichlet-multinomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log density of the Dirichlet-multinomial distribution for one data point $\\mathbf{x}$ is shown as follow:\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\alpha) \n",
    "&= \\ln \\binom{|\\mathbf{x}|}{\\mathbf{x}} +  \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{j}) - \\ln \\Gamma(\\alpha_j)] - [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}|) - \\ln \\Gamma(|\\alpha|)]\\\\\n",
    "&=  \\ln |\\mathbf{x}|! - \\ln(x_1!)-...-\\ln(x_{64}!)+ \\sum_{j=1}^{64} [\\ln \\Gamma(\\alpha_j + x_{j}) - \\ln \\Gamma(\\alpha_j)] - [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}|) - \\ln \\Gamma(|\\alpha|)]\\\\\n",
    "&=  \\ln |\\mathbf{x}|! - [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}|) - \\ln \\Gamma(|\\alpha|)]+ \\sum_{j=1}^{64} [\\ln \\Gamma(\\alpha_j + x_{j}) - \\ln \\Gamma(\\alpha_j)-\\ln(x_j!)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_logpdf(x::Vector, α::Vector)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at data point `x`.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(x::Vector, α::Vector)\n",
    "    # store the sum of x and α\n",
    "    sumx = sum(x)\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    # store length of x\n",
    "    lengthx = length(x) \n",
    "    \n",
    "    # part without summation\n",
    "    logden = lfact(sumx) - lgamma(sumα + sumx) + lgamma(sumα)\n",
    "    \n",
    "    # part within summation\n",
    "    for j in 1:lengthx\n",
    "       logden += lgamma(α[j] + x[j]) - lgamma(α[j]) - lfact(x[j])  \n",
    "    end\n",
    "    \n",
    "    # return log density\n",
    "    return logden\n",
    "end\n",
    "\n",
    "function dirmult_logpdf!(r::Vector, X::Matrix, α::Vector)\n",
    "    for j in 1:size(X, 2)\n",
    "        r[j] = dirmult_logpdf(X[:, j], α)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at each data point in `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::Matrix, α::Vector)\n",
    "    r = zeros(size(X, 2))\n",
    "    dirmult_logpdf!(r, X, α)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Read in `optdigits.tra`, the training set of 3823 handwritten digits. Each row contains the 64 counts of a digit and the last element (65th element) indicates what digit it is. For grading purpose, evaluate the total log-likelihood of this data at parameter values $\\alpha=(1,\\ldots,1)$ using your function in Q3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in train data\n",
    "path1_tra = \"http://hua-zhou.github.io/teaching/biostatm280\"\n",
    "path2_tra = \"-2018spring/hw/hw4/optdigits.tra\"\n",
    "\n",
    "train = readdlm(download(path1_tra * path2_tra), ',')\n",
    "\n",
    "# separte last column which store the digit \n",
    "train_x = round.(Int64, train[:, 1:64])\n",
    "train_digit = round.(Int64, train[:, 65]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-638817.993292528"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store size of train data\n",
    "length_x = size(train_x, 2)\n",
    "length_obs = size(train_x, 1)\n",
    "\n",
    "# input α vector\n",
    "α = round.(Int64, ones(length_x))\n",
    "\n",
    "sum(dirmult_logpdf(train_x', α))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The total log-likelihood of this data at parameter values $\\alpha=(1,\\ldots,1)$ is around -638818."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question5 \n",
    "Derive the score function $\\nabla L(\\alpha)$, observed information matrix $-d^2L(\\alpha)$, and Fisher information matrix $\\mathbf{E}[-d^2L(\\alpha)]$ for the Dirichlet-multinomial distribution.\n",
    "\n",
    "Comment on why Fisher scoring method is inefficient for computing MLE in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "* **score function**\n",
    "\n",
    "From part 2, we have \n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)]\n",
    "$$\n",
    "By taking first derivative of $L(\\alpha)$ respect to each element in vector $\\alpha$, we get the socre function to be as follow,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla L(\\alpha) \n",
    "&= \\sum_{i = 1}^n (\\Psi(\\alpha_j+x_{ij})-\\Psi(\\alpha_j)) - \\sum_{i=1}^n(\\Psi(|\\alpha|+|x_i|)-\\Psi(|\\alpha|))\\\\\n",
    "&= \\sum_{i = 1}^n (\\Psi(\\alpha_j+x_{ij})-\\Psi(\\alpha_j)) - \\sum_{i=1}^n\\Psi(|\\alpha|+|x_i|)+n\\Psi(|\\alpha|)\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\Psi(x)= \\frac{d}{dx}log\\Gamma(x)$ is the digamma function.\n",
    "* **observed information matrix**\n",
    "\n",
    "$$\n",
    "-d^2L(\\alpha)=-\\frac{d\\nabla L(\\alpha)}{d\\alpha}\n",
    "=\\sum_{i = 1}^n[\\Psi'(\\alpha_j)-\\Psi'(\\alpha_j+x_{ij})]-\\sum_{i = 1}^n[\\Psi'(|\\alpha|)-\\Psi'(|\\alpha|+|x_i|)]\n",
    "$$\n",
    "(1) diagonal matrix part, $D$\n",
    "\n",
    "When taking the second derivative respect to $\\alpha$, the terms with only $\\alpha_j$ will pretain if both of the derivatives taken respect to $\\alpha_j$. That is $\\Psi'(\\alpha_j)$ and $\\Psi'(\\alpha_j+x_{ij})$ terms will only appear on the diaganol of observed information matrix. Therefore, let's define a diagonal matrix $D$, which has the $j^{th}$diagonal entry shown as\n",
    "$\\sum_{i = 1}^n[\\Psi'(\\alpha_j)-\\Psi'(\\alpha_j+x_{ij})]$.\n",
    "From recurrence relation of polygamma function $\\Psi^{(m)}(z+1)=\\Psi^{(m)}(z)+\\frac{(-1)^mm!}{z^{m+1}}$, the trigamma function$\\Psi'(z+1)=\\Psi^{(1)}(z+1)=\\Psi^{(1)}(z)+\\frac{(-1)^mm!}{z^{m+1}}$.\n",
    "\n",
    "Then,\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\Psi^{(1)}(\\alpha_j+1)=\\Psi^{(1)}(\\alpha_j)-\\frac{1}{\\alpha_j^2}\\\\\n",
    "&\\Psi^{(1)}(\\alpha_j+2)=\\Psi^{(1)}(\\alpha_j+1)-\\frac{1}{\\alpha_j^2+1}\\\\\n",
    "&.\\\\\n",
    "&.\\\\\n",
    "&.\\\\\n",
    "&\\Psi^{(1)}(\\alpha_j+x_{ij})=\\Psi^{(1)}(\\alpha_j+x_{ij}-1)-\\frac{1}{(\\alpha_j+x_{ij}-1)^2}\n",
    "\\end{align}\n",
    "$$\n",
    "By suming all the above equations, we get\n",
    "$\\Psi'(\\alpha_j)-\\Psi'(\\alpha_j+x_{ij})=\\sum_{p=0}^{x_{ij}-1} \\frac{1}{(\\alpha_j+p)^2}$\n",
    "Therefore, the $j_{th}$ diagonal position in matrix $D$, \n",
    "$$\\sum_{i = 1}^n[\\Psi'(\\alpha_j)-\\Psi'(\\alpha_j+x_{ij})]=\\sum_{i = 1}^n\\sum_{p=0}^{x_{ij}-1} \\frac{1}{(\\alpha_j+p)^2}$$\n",
    "(2) other part, $m$\n",
    "\n",
    "Similarly, the terms left in the observated information matrix is \n",
    "$$\n",
    "m= \\sum_{i = 1}^n[\\Psi'(|\\alpha|)-\\Psi'(|\\alpha|+|x_i|)]\n",
    "= \\sum_{i = 1}^n\\sum_{p=0}^{|x_{i}|-1} \\frac{1}{(|\\alpha|+p)^2}\n",
    ",$$ which is a constant over the whole matrix.\n",
    "\n",
    "Therefore, the observed info matrix can be written as \n",
    "$\\mathbf{D}-m\\mathbf{1}\\mathbf{1}'$, where $\\mathbf{D}$ and $m$ are defined previously.\n",
    "\n",
    "* **Fisher information matrix $\\mathbf{E}[-d^2L(\\alpha)]$ **\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{E}[-d^2L(\\alpha)]\n",
    "&=\\mathbf{E}(\\mathbf{D}-m\\mathbf{1}\\mathbf{1}')\\\\\n",
    "&=\\mathbf{E}(\\mathbf{D})-m\\mathbf{1}\\mathbf{1}'\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "For calculating the expected value, we need to integrate over $\\alpha$, which does not have analytical solution. Therefore, Fisher scoring method is insufficient for computing MLE in this example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What structure does the observed information matrix possess that can facilitate the evaluation of the Newton direction? Is the observed information matrix always positive definite? What remedy can we take if it fails to be positive definite? (Hint: HW1 Q6.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Solution**\n",
    "\n",
    "1.special structure\n",
    "\n",
    "From part 5, we can see the observed information matrix $[-d^2L(\\alpha)]=\\mathbf{D}-m\\mathbf{1}\\mathbf{1}'$, where $D$ is a diagonal matrix with $j^{th}$ diagonal entry, $\\sum_{i = 1}^n\\sum_{p=0}^{x_{ij}-1} \\frac{1}{(\\alpha_j+p)^2}$ and $m=\\sum_{i = 1}^n\\sum_{p=0}^{|x_{i}|-1} \\frac{1}{(|\\alpha|+p)^2}$ is a constant. Therefore the observed information matrix is composed by a diagonal matrix and a rank one matrix. \n",
    "\n",
    "2.not always positive difinite \n",
    "$$\n",
    "\\begin{align}\n",
    "det[-d^2L(\\alpha)] \n",
    "&= det[\\mathbf{D}-m\\mathbf{1}\\mathbf{1}'] \\\\\n",
    "&= det[\\mathbf{D}(\\mathbf{I}-\\mathbf{D}^{-1}m\\mathbf{1}\\mathbf{1}')] \\\\\n",
    "&= det(\\mathbf{D})det(\\mathbf{I}-m\\mathbf{D}^{-1}\\mathbf{1}\\mathbf{1}')\\\\\n",
    "&= (\\prod_{j}d_j)(1-\\sum_j\\frac{m}{d_j})\n",
    "\\end{align}\n",
    "$$\n",
    "In order to make the matrix positive definite, the determant of the matrix should be positive. From the above equation, that is $1-\\sum_j\\frac{m}{d_j}>0$ should be satisfied. However, it is not always the case. Therefore, the matrix is not necessarily positive definite.\n",
    "\n",
    "3.remedy for pd\n",
    "\n",
    "In order to make the matrix positive definite, $1-\\sum_j\\frac{m}{d_j}>0$ should be satisfied. That is, $m<\\frac{1}{\\sum_j(dj^{-1})}$\n",
    "\n",
    "Therefore, we can take the remedy according to the following rules,\n",
    "$$\n",
    "m_{new} = \n",
    "\\begin{cases}\n",
    "m & \\text{if } m<\\frac{1}{\\sum_j(dj^{-1})}\\\\\n",
    "0.9 \\frac{1}{\\sum_j(dj^{-1})}& \\text{o.w.}  \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "4.facillitate the evaluation of the Newton direction.\n",
    "\n",
    "\n",
    "With the Sherman-Morrison formula, $$\n",
    "\t(\\mathbf{A} + \\mathbf{u} \\mathbf{u}^T)^{-1} = \\mathbf{A}^{-1} - \\frac{1}{1 + \\mathbf{u}^T \\mathbf{A}^{-1} \\mathbf{u}} \\mathbf{A}^{-1} \\mathbf{u} \\mathbf{u}^T \\mathbf{A}^{-1},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "[-d^2L(\\alpha)]^{-1}\n",
    "&= [\\mathbf{D}-m\\mathbf{1}\\mathbf{1}']^{-1}\\\\\n",
    "&= \\mathbf{D}^{-1}+\\frac{m}{1+m\\mathbf{1}'\\mathbf{D}^{-1}\\mathbf{1}}\\mathbf{D}^{-1}\\mathbf{1}\\mathbf{1}'\\mathbf{D}^{-1}\\\\\n",
    "&= \\mathbf{D}^{-1}+\\frac{1}{m^{-1}+\\mathbf{1}'\\mathbf{D}^{-1}\\mathbf{1}}\\mathbf{D}^{-1}\\mathbf{1}\\mathbf{1}'\\mathbf{D}^{-1}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In this way, the inverse of observed info matrix can be decomposed to calculate the inverse of a diagonal matrix $\\mathbf{D}$. In this way, the evaluation of the Newton direction can be facillitated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Discuss how to choose a good starting point. Implement this as the default starting value in your function below. (Hint: Method of moment estimator may furnish a good starting point.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Solution**\n",
    "\n",
    "From the Dirichlet distruibution, we have the first and second theoritical moments shown as below,\n",
    "$$\n",
    "\\mathbf{E}(p_k)=\\frac{\\alpha_k}{|\\alpha|}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{E}(p_k^2)=\\frac{\\alpha_k(\\alpha_k+1)}{|\\alpha|(1+|\\alpha|)}\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\sum_k\\frac{\\mathbf{E}(p_j^2)}{\\mathbf{E}(p_j)} \n",
    "= \\sum_k\\frac{\\frac{\\alpha_k(\\alpha_k+1)}{|\\alpha|(1+|\\alpha|)}}{\\frac{\\alpha_k}{|\\alpha|}}\n",
    "= \\sum_k\\frac{\\alpha_k+1}{1+|\\alpha|}\n",
    "= \\frac{|\\alpha|+d}{|\\alpha|+1}\n",
    "$$\n",
    "From the n observations, we have the first and second sample moments shown as below,\n",
    "$$\n",
    "\\mathbf{E}(p_k)=\\frac{1}{n}\\sum_{i=1}^n\\frac{x_{ik}}{|x_i|}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{E}(p_k^2)=\\frac{1}{n}\\sum_{i=1}^n(\\frac{x_{ik}}{|x_i|})^2\n",
    "$$\n",
    "then let constant c defined as follow,\n",
    "$$\n",
    "c=\\sum_k\\frac{\\mathbf{E}(p_j^2)}{\\mathbf{E}(p_j)} \n",
    "=\\sum_k\\frac{\\frac{1}{n}\\sum_{i=1}^n(\\frac{x_{ik}}{|x_i|})^2}{\\frac{1}{n}\\sum_{i=1}^n\\frac{x_{ik}}{|x_i|}}\n",
    "=\\sum_k\\frac{\\sum_{i=1}^n(\\frac{x_{ik}}{|x_i|})^2}{\\sum_{i=1}^n\\frac{x_{ik}}{|x_i|}}\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "\\frac{|\\alpha|+d}{|\\alpha|+1}=c\\\\\n",
    "(c+1)|\\alpha|=d-c\\\\\n",
    "|\\alpha|=\\frac{d-c}{c-1}\n",
    "$$\n",
    "In order to get each elements in $\\alpha$, \n",
    "$$\n",
    "\\alpha_k = |\\alpha|\\mathbf{E}(p_k)=\\frac{d-c}{c-1}\\frac{\\sum_i x_{ik}}{\\sum_i|\\mathbf{x_i}|}\n",
    "$$\n",
    "where $c=\\sum_k\\frac{\\sum_{i=1}^n(\\frac{x_{ik}}{|x_i|})^2}{\\sum_{i=1}^n\\frac{x_{ik}}{|x_i|}}$\n",
    "\n",
    "In this way, we can use the given data $x_{ik}$ where $i$ from 1 to number of all data points and $k$ is from 1 to $d=64$ to calculate the initial $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using the Newton's method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **function to calculate score**\n",
    "\n",
    "$$\n",
    "\\nabla L(\\alpha) \n",
    "= \\sum_{i = 1}^n (\\Psi(\\alpha_j+x_{ij})-\\Psi(\\alpha_j)) - \\sum_{i=1}^n(\\Psi(|\\alpha|+|x_i|)-\\Psi(|\\alpha|))\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_score (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pkg.add(\"SpecialFunctions.jl\")\n",
    "using SpecialFunctions\n",
    "\n",
    "# define the function to calculate score function of log-likelihood\n",
    "# take column of X as one data point\n",
    "function dirmult_score(X::Matrix, α::Vector)\n",
    "    # initialize output \n",
    "    score = zeros(length(α))\n",
    "    \n",
    "    # sum of alpha vector\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    # column sum of X\n",
    "    colsumX = sum(X, 1)\n",
    "    \n",
    "    # store the size of n and d\n",
    "    n = size(X, 2)\n",
    "    d = size(X, 1)\n",
    "    \n",
    "    # evalutate score function at input α\n",
    "    for j in 1:d\n",
    "        for i in 1:n\n",
    "            score[j] += digamma(α[j] + X[j, i]) - digamma(α[j]) - \n",
    "                digamma(sumα + colsumX[i]) + digamma(sumα)\n",
    "            end \n",
    "    end\n",
    "    \n",
    "    # return vector score\n",
    "    return score \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64-element Array{Float64,1}:\n",
       " -6809.0  \n",
       " -5994.02 \n",
       "   399.53 \n",
       "  4465.77 \n",
       "  4255.45 \n",
       "  -239.022\n",
       " -4871.03 \n",
       " -6583.75 \n",
       " -6804.21 \n",
       " -3509.27 \n",
       "  3477.72 \n",
       "  4619.83 \n",
       "  3901.48 \n",
       "     ⋮    \n",
       "  3294.76 \n",
       "  2375.96 \n",
       " -2036.58 \n",
       " -6477.55 \n",
       " -6808.0  \n",
       " -6101.94 \n",
       "   525.922\n",
       "  4463.0  \n",
       "  3988.35 \n",
       "   655.835\n",
       " -3961.5  \n",
       " -6442.54 "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for score function\n",
    "dirmult_score(train_x', α)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **function to calculate observed info matrix**\n",
    "\n",
    "The observed info matrix can be written as $\\mathbf{D}-m\\mathbf{1}\\mathbf{1}'$, where $\\mathbf{D}$ and $m$ are defined as follow:\n",
    "\n",
    "$j_{th}$ diagonal position in matrix $D$,\n",
    "$$d_j=\\sum_{i = 1}^n[\\Psi'(\\alpha_j)-\\Psi'(\\alpha_j+x_{ij})]$$\n",
    "and \n",
    "$$\n",
    "m= \\sum_{i = 1}^n[\\Psi'(|\\alpha|)-\\Psi'(|\\alpha|+|x_i|)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_obs (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to calculate the observed information matrix \n",
    "# return diagonal vector d and constant m for recovery\n",
    "# take column of X as one data point\n",
    "function dirmult_obs(X::Matrix, α::Vector)\n",
    "    # obs matrix = Diagonal(d) - m11'\n",
    "    # initialization \n",
    "    d = zeros(length(α))\n",
    "    m = 0.0\n",
    "    \n",
    "    # store the size of n and number of parameters\n",
    "    n = size(X, 2)\n",
    "    par = size(X, 1)\n",
    "    \n",
    "    # sum of alpha vector\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    # column sum of X\n",
    "    colsumX = sum(X, 1)\n",
    "    \n",
    "    # evaluation d \n",
    "    for j in 1:par\n",
    "        for i in 1:n\n",
    "            d[j] += trigamma(α[j]) - trigamma(α[j] + X[j, i])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # evaluation m\n",
    "    for i in 1:n\n",
    "        m += trigamma(sumα) - trigamma(sumα + colsumX[i])\n",
    "    end\n",
    "    \n",
    "    return m, d\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50.0163869748493, [0.0, 688.915, 4313.97, 5730.92, 5665.9, 3860.62, 1242.08, 151.565, 3.71361, 2231.94  …  2853.36, 255.11, 1.0, 574.829, 4306.32, 5688.55, 5469.75, 4162.56, 1787.01, 264.045])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for obs matrix fcn\n",
    "dirmult_obs(train_x', α)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **initial $\\mathbf{α}$**\n",
    "\n",
    "The $k^{th}$ element in initial $\\mathbf{α}$ is \n",
    "$$\n",
    "\\alpha_k = \\frac{d-c}{c-1}\\frac{\\sum_i x_{ik}}{\\sum_i|\\mathbf{x_i}|}\n",
    "$$\n",
    "where $c=\\sum_k\\frac{\\sum_{i=1}^n(\\frac{x_{ik}}{|x_i|})^2}{\\sum_{i=1}^n\\frac{x_{ik}}{|x_i|}}$\n",
    "and \n",
    "$d$ is the length of $\\mathbf{α}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_α0 (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the function to calculate the initial value for α\n",
    "# take column of X as one data point\n",
    "function dirmult_α0(X::Matrix)\n",
    "    # initialization\n",
    "    α0 = zeros(length(α))\n",
    "    c = 0.0\n",
    "    \n",
    "    # store the size of n and number of parameters \n",
    "    n = size(X, 2)\n",
    "    d = size(X, 1)\n",
    "    \n",
    "    # sum of alpha vector\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    # column sum of X\n",
    "    colsumX = sum(X, 1)\n",
    "    \n",
    "    # total of X\n",
    "    total = sum(colsumX)\n",
    "    \n",
    "    # evaluate c \n",
    "    for k in 1:d\n",
    "        denumerator = sum(X[k, :] ./ colsumX)\n",
    "        if  denumerator > 0\n",
    "            c += sum(X[k, :].^2 ./ colsumX.^2) / denumerator \n",
    "        end\n",
    "    end\n",
    "    \n",
    "        \n",
    "    # evaluate α0 and make sure αsum is nonegative\n",
    "    α0 = max((d - c) / (c - 1), 1e-4) / total * sum(X, 2)\n",
    "    \n",
    "    return α0\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64×1 Array{Float64,2}:\n",
       " 0.0        \n",
       " 0.0683308  \n",
       " 1.24306    \n",
       " 2.67712    \n",
       " 2.59675    \n",
       " 1.2484     \n",
       " 0.314606   \n",
       " 0.0322673  \n",
       " 0.000474519\n",
       " 0.444565   \n",
       " 2.39852    \n",
       " 2.6566     \n",
       " 2.40931    \n",
       " ⋮          \n",
       " 2.21428    \n",
       " 2.10509    \n",
       " 0.848974   \n",
       " 0.0336316  \n",
       " 5.93149e-5 \n",
       " 0.0641787  \n",
       " 1.32788    \n",
       " 2.7082     \n",
       " 2.59894    \n",
       " 1.51941    \n",
       " 0.477485   \n",
       " 0.0458504  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for initial alpha\n",
    "dirmult_α0(train_x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **implement of Newton's method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_newton"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_newton(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using Newton's method.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `n`-by-`d` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: a `d` vector of starting point (optional). \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "* `maximum`: the log-likelihood at MLE.   \n",
    "* `estimate`: the MLE. \n",
    "* `gradient`: the gradient at MLE. \n",
    "* `hessian`: the Hessian at MLE. \n",
    "* `se`: a `d` vector of standard errors. \n",
    "* `iterations`: the number of iterations performed.\n",
    "\"\"\"\n",
    "\n",
    "function dirmult_newton(X::Matrix; α0::Vector{Float64} = vec(dirmult_α0(X)), \n",
    "            maxiters::Int = 100, tolfun::Float64 = 1e-8)\n",
    "    # store original size of α\n",
    "    α_d = size(X, 1)\n",
    "    \n",
    "    # remove rows with all 0 entries\n",
    "    row_ind = find(sum(X, 2))\n",
    "    \n",
    "    # get the new X removing all the 0 sum lines\n",
    "    X_nz = X[row_ind, :]\n",
    "    \n",
    "    # store the number of parameters \n",
    "    d = size(X_nz, 1)\n",
    "    \n",
    "    # initialization\n",
    "    α_old = α0[row_ind]\n",
    "    \n",
    "    logl_old = sum(dirmult_logpdf(X_nz, α_old))\n",
    "    \n",
    "    # pre-allocation \n",
    "    α_new = zeros(d)\n",
    "    gradient = zeros(d)\n",
    "    \n",
    "    logl_new = 0.0\n",
    "    newtondir = zeros(d)\n",
    "    \n",
    "    # obs matrix = diagonal(D_diag) - m11'\n",
    "    D_diag = zeros(d)\n",
    "    m = 0.0\n",
    "    # check for pd remedy \n",
    "    m_bound = 0.0 \n",
    "    \n",
    "    # iteration count\n",
    "    iteration = 0\n",
    "    \n",
    "    # Newton loop\n",
    "    for iter in 1:maxiters\n",
    "        # evaluate gradient (score)\n",
    "        gradient = dirmult_score(X_nz, α_old)\n",
    "        \n",
    "        # compute inverse obs matrix\n",
    "        m, D_diag = dirmult_obs(X_nz, α_old)\n",
    "        \n",
    "        # remedy for making sure obs matrix is pd\n",
    "        sum_dinv = sum(1 ./ D_diag)\n",
    "        \n",
    "        if m >= 1 / sum_dinv\n",
    "            m = 0.95 * 1 / sum_dinv\n",
    "        end\n",
    "        \n",
    "        # compute Newton's direction based on Sherman-Morrison formula\n",
    "        newtondir .= (1 ./ D_diag) .* gradient + 1 / (1 / m - sum_dinv) *\n",
    "            ((1 ./ D_diag).^2) .* gradient\n",
    "        \n",
    "        \n",
    "        # check nonegativity condition for step halving process\n",
    "        step = 1.0\n",
    "            \n",
    "        for i in 1:d\n",
    "            if newtondir[i] < 0\n",
    "               step = min(step, abs(α_old[i] / newtondir[i]) * 0.95)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # line search loop\n",
    "        for lsiter in 1:10\n",
    "            # step halving\n",
    "            step = step / 2^(lsiter - 1)\n",
    "            \n",
    "            # get new α\n",
    "            α_new .= α_old + step .* newtondir\n",
    "            logl_new = sum(dirmult_logpdf(X_nz, α_new))\n",
    "            \n",
    "            # check for getting higher loglikelihood fcn\n",
    "            if logl_new > logl_old\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # check convergence criterion\n",
    "        if abs(logl_new - logl_old) < tolfun * (abs(logl_old) + 1)\n",
    "            # update before break the loop\n",
    "            α_old = copy(α_new)\n",
    "            logl_old = logl_new\n",
    "            iteration += 1\n",
    "            break;\n",
    "        end\n",
    "        \n",
    "        # update \n",
    "        α_old = copy(α_new)\n",
    "        logl_old = logl_new\n",
    "        iteration += 1\n",
    "        \n",
    "    end\n",
    "    \n",
    "    # add zeros back to α vector\n",
    "    α = zeros(α_d)\n",
    "    α[row_ind] = α_old[1:d]\n",
    "\n",
    "    \n",
    "    \n",
    "    # compute logl, gradient, Hessian from final iterate\n",
    "    #logl = sum(dirmult_logpdf(X_nz, α_old))\n",
    "    gradient = dirmult_score(X_nz, α_old)\n",
    "    m, D_diag = dirmult_obs(X_nz, α_old)\n",
    "    hessian = (Diagonal(vec(D_diag)) - m * ones(d, d)) * (-1)\n",
    "    \n",
    "    # printout \n",
    "    print(\"In iteratiton $iteration, the log likelihood is $logl_old.\\n\")\n",
    "\n",
    "    # output\n",
    "    return α, logl_old, gradient, hessian, m, D_diag, iteration\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Read in `optdigits.tra`, the training set of 3823 handwritten digits. Find the MLE for the subset of digit 0, digit 1, ..., and digit 9 separately using your function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in train data\n",
    "path1_tra = \"http://hua-zhou.github.io/teaching/biostatm280\"\n",
    "path2_tra = \"-2018spring/hw/hw4/optdigits.tra\"\n",
    "\n",
    "train = readdlm(download(path1_tra * path2_tra), ','); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In iteratiton 100, the log likelihood is -37358.44477239687.\n",
      "In iteratiton 35, the log likelihood is -42179.24637973165.\n",
      "In iteratiton 36, the log likelihood is -39985.26535316699.\n",
      "In iteratiton 52, the log likelihood is -40519.47261212615.\n",
      "In iteratiton 20, the log likelihood is -43488.77396892435.\n",
      "In iteratiton 30, the log likelihood is -41191.310187595074.\n",
      "In iteratiton 50, the log likelihood is -37702.51164683071.\n",
      "In iteratiton 30, the log likelihood is -40303.998120447286.\n",
      "In iteratiton 38, the log likelihood is -43130.84885933372.\n",
      "In iteratiton 30, the log likelihood is -43709.654694763056.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64×10 Array{Float64,2}:\n",
       "  0.0        0.0         0.0         …  0.0         0.0         0.0       \n",
       "  0.0373906  0.00802212  0.388782       0.139553    0.0878938   0.0764991 \n",
       "  4.98341    0.513663    3.81674        2.49385     2.49388     1.53207   \n",
       " 14.869      2.09636     5.24514        5.05273     5.82405     3.70945   \n",
       " 12.1413     3.05364     2.51117        5.44493     5.57322     3.74211   \n",
       "  2.45072    1.31993     0.310891    …  4.45732     2.41909     1.51545   \n",
       "  0.0634241  0.137704    0.0157806      1.71728     0.199493    0.298883  \n",
       "  0.0        0.0         0.0            0.178769    0.0         0.0159564 \n",
       "  0.0        0.0         0.0            0.0         0.00219556  0.0       \n",
       "  1.02159    0.0572512   1.73373        0.25464     1.02138     0.667673  \n",
       " 14.5709     1.03971     5.50033     …  3.17203     6.09404     4.0291    \n",
       " 14.7314     3.23649     4.75357        3.98638     4.55618     3.3561    \n",
       " 13.7197     4.1002      4.44346        4.16575     4.08026     3.29635   \n",
       "  ⋮                                  ⋱                                    \n",
       " 11.7051     4.04494     4.39691        1.55561     3.34038     2.22633   \n",
       " 15.2128     2.27057     3.8434         0.0531101   4.0281      2.62321   \n",
       "  2.58562    0.272652    2.36769        0.00191345  0.77511     0.98585   \n",
       "  0.0        0.03031     0.136188    …  0.0         0.0         0.0159951 \n",
       "  0.0        0.0         0.00193679     0.0         0.0         0.0       \n",
       "  0.0207335  0.0177323   0.355146       0.0991496   0.0779233   0.053616  \n",
       "  5.01667    0.514227    3.88746        2.83519     2.57005     1.46766   \n",
       " 15.3498     2.0167      5.26543        4.11543     6.16012     3.77261   \n",
       " 15.0482     3.51897     4.93814     …  0.451037    5.70334     3.56516   \n",
       "  5.81179    2.20591     4.52149        0.0117565   2.68656     1.61839   \n",
       "  0.194096   0.408016    3.02154        0.0         0.299542    0.336046  \n",
       "  0.0        0.0419013   0.33496        0.0         0.00659738  0.00702595"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the training data set to Int64\n",
    "X = round.(Int64, train);\n",
    "\n",
    "# initialize matrix to store \n",
    "α_all = zeros(64, 10) \n",
    "\n",
    "for digit in 0:9\n",
    "    # get the train_x subset for different digits\n",
    "    X_digit = (X[X[:, 65] .== digit, 1:64])'\n",
    "    \n",
    "    # store the MLE for α\n",
    "    α_all[:, (digit + 1)] .= dirmult_newton(X_digit)[1]\n",
    "end\n",
    "\n",
    "α_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10\n",
    "\n",
    "As $\\alpha / |\\alpha| \\to \\mathbf{p}$, the Dirichlet-multinomial distribution converges to a multinomial with parameter $\\mathbf{p}$. Therefore multinomial can be considered as a special Dirichlet-multinomial with $|\\alpha|=\\infty$. Perform a likelihood ratio test (LRT) whether Dirichlet-multinomial offers a better fit than multinomial for digits 0, 1, ..., 9 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Solution**\n",
    "\n",
    "In this question, we try to compare the Dirichelt-multinomial distribution with multinomial using the Likelihood ratio test. In multinomial we have $\\mathbf{p}$ as parameter vector, which has $d$ components. In Dirichlet-multinomial distribution we have $\\mathbf{\\alpha}$ as the parameter vector, but we also have $\\sum_j{p_j}=1$ as an extra constraint. Thus, we have $(d-1)$ parameters in the Dirichelet-multinomial distribution. Therefore, the difference of parameter space is 1. \n",
    "\n",
    "Based on the LRT, we have $H_0$: *reduced model(multinomial) is true* \n",
    "vs\n",
    "$H_A:$ *full model(Dirichlet-multinomial) is true*  \n",
    "\n",
    "$$\n",
    "\\Delta{G^2} = -2log(L_{Dirchilet}-L_{multinomial})\\sim \\chi_1^2\n",
    "$$\n",
    "Therefore, we need to check the p-value of the $\\chi_1^2$ distribution for the LRT, in order to decide whether Dirichelet-multinomial offers a better fit than multinomial for digits from 0 to 9 respectively.\n",
    "\n",
    "For Dirchilet-multinomial distribution, we can get the loglikelihood for each digit from `dirmult_newton` function.\n",
    "\n",
    "For multinomial distribution, the loglikelihood function is\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\mathbf{p}|\\mathbf{x}) \n",
    "&= \\log\\{\\frac{(\\sum x_i)!}{\\prod{x_i!}}\\prod{p_i^{x_i}} \\} \\\\\n",
    "&= \\log(\\sum x_i)!-\\sum \\log(x_i!)+\\sum x_i \\log(p_i)\n",
    "\\end{align}\n",
    "$$\n",
    "By posing all the first derivative to be 0, we get the maximum likelihood estimators\n",
    "$$\n",
    "\\hat{p}_i=\\frac{x_i}{\\sum x_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For digit 0, the loglikehihood basd on Dirichlet multinomial is -37358.44477239687. \n",
      "For digit 0, the loglikehihood basd on multinomial is -39592.19957521694. \n",
      "For digit 0, the p-value is 0.0 < 0.5, which means Dirichlet multinomial is a better fit.\n",
      "For digit 1, the loglikehihood basd on Dirichlet multinomial is -42179.24637973165. \n",
      "For digit 1, the loglikehihood basd on multinomial is -93631.42512195373. \n",
      "For digit 1, the p-value is 0.0 < 0.5, which means Dirichlet multinomial is a better fit.\n",
      "For digit 2, the loglikehihood basd on Dirichlet multinomial is -39985.26535316699. \n",
      "For digit 2, the loglikehihood basd on multinomial is -142742.877266636. \n",
      "For digit 2, the p-value is 0.0 < 0.5, which means Dirichlet multinomial is a better fit.\n",
      "For digit 3, the loglikehihood basd on Dirichlet multinomial is -40519.47261212615. \n",
      "For digit 3, the loglikehihood basd on multinomial is -189831.95639558806. \n",
      "For digit 3, the p-value is 0.0 < 0.5, which means Dirichlet multinomial is a better fit.\n",
      "For digit 4, the loglikehihood basd on Dirichlet multinomial is -43488.77396892435. \n",
      "For digit 4, the loglikehihood basd on multinomial is -247176.03650506484. \n",
      "For digit 4, the p-value is 0.0 < 0.5, which means Dirichlet multinomial is a better fit.\n",
      "For digit 5, the loglikehihood basd on Dirichlet multinomial is -41191.310187595074. \n",
      "For digit 5, the loglikehihood basd on multinomial is -298889.0329614315. \n",
      "For digit 5, the p-value is 0.0 < 0.5, which means Dirichlet multinomial is a better fit.\n",
      "For digit 6, the loglikehihood basd on Dirichlet multinomial is -37702.51164683071. \n",
      "For digit 6, the loglikehihood basd on multinomial is -341486.3371545706. \n",
      "For digit 6, the p-value is 0.0 < 0.5, which means Dirichlet multinomial is a better fit.\n",
      "For digit 7, the loglikehihood basd on Dirichlet multinomial is -40303.998120447286. \n",
      "For digit 7, the loglikehihood basd on multinomial is -390959.3183912035. \n",
      "For digit 7, the p-value is 0.0 < 0.5, which means Dirichlet multinomial is a better fit.\n",
      "For digit 8, the loglikehihood basd on Dirichlet multinomial is -43130.84885933372. \n",
      "For digit 8, the loglikehihood basd on multinomial is -440655.1690156729. \n",
      "For digit 8, the p-value is 0.0 < 0.5, which means Dirichlet multinomial is a better fit.\n",
      "For digit 9, the loglikehihood basd on Dirichlet multinomial is -43709.654694763056. \n",
      "For digit 9, the loglikehihood basd on multinomial is -495232.95521823555. \n",
      "For digit 9, the p-value is 0.0 < 0.5, which means Dirichlet multinomial is a better fit.\n"
     ]
    }
   ],
   "source": [
    "using Distributions\n",
    "\n",
    "# convert the training data set to Int64\n",
    "X = round.(Int64, train);\n",
    "\n",
    "# initialization \n",
    "logl_Dirichlet = 0.0\n",
    "logl_multi = 0.0\n",
    "p_value = 0.0\n",
    "α_digit = zeros(64, 1)\n",
    "\n",
    "\n",
    "# find p-value for each model\n",
    "for digit in 0:9\n",
    "    # get the train_x subset for different digits\n",
    "    X_digit = (X[X[:, 65] .== digit, 1:64])'\n",
    "    \n",
    "    # remove rows with all 0 entries\n",
    "    row_ind = find(sum(X_digit, 2))\n",
    "    \n",
    "    # get the new X removing all the 0 sum lines\n",
    "    X_nz = X_digit[row_ind, :]\n",
    "        \n",
    "    # store α for each digit\n",
    "    α_digit .= α_all[:, (digit + 1)]\n",
    "    \n",
    "    # store the nonzero elements in α_nz\n",
    "    α_nz = α_digit[row_ind]\n",
    "    \n",
    "    # calculate Dirichlet multinomial loglikelihood  \n",
    "    logl_Dirichlet = sum(dirmult_logpdf(X_nz, α_nz))\n",
    "    \n",
    "    print(\"For digit $digit, the loglikehihood basd on Dirichlet \" * \n",
    "        \"multinomial is $logl_Dirichlet. \\n\")\n",
    "    \n",
    "    # multinomial MLE \n",
    "    xi = sum(X_nz, 2)\n",
    "    p_hat = xi / sum(xi)\n",
    "    \n",
    "    # number of data points\n",
    "    n = size(X_nz, 2)\n",
    "    \n",
    "    # sum of x for each data point\n",
    "    sum_data = round.(Int64, sum(X_nz, 1))\n",
    "    \n",
    "    # add up logpdf for all the data points \n",
    "    for i in 1:n\n",
    "        logl_multi += logpdf(Multinomial(sum_data[i], vec(p_hat)),\n",
    "            X_nz[:, i])\n",
    "    end\n",
    "    \n",
    "    print(\"For digit $digit, the loglikehihood basd on \" *\n",
    "        \"multinomial is $logl_multi. \\n\")\n",
    "    \n",
    "    # chi square 1 p-value\n",
    "    p_value = 1 - cdf(Chisq(1), -2 * (logl_multi - logl_Dirichlet))\n",
    "    \n",
    "    if p_value < 0.5\n",
    "        print(\"For digit $digit, the p-value is $p_value < 0.5, \" *\n",
    "        \"which means Dirichlet multinomial is a better fit.\\n\")\n",
    "    else \n",
    "        print(\"For digit $digit, the p-value is $p_value < 0.5, \" *\n",
    "        \"which means multinomial is a better fit.\\n\")\n",
    "    end\n",
    "end\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the above results, for each digit, Dirichlet multinomial is a better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "Now we can construct a simple Bayesian rule for handwritten digits recognition:\n",
    "$$\n",
    "\t\\mathbf{x}\t\\mapsto \\arg \\max_k \\widehat \\pi_k f(x|\\widehat \\alpha_k).\n",
    "$$\n",
    "Here we can use the proportion of digit $k$ in the training set as the prior probability $\\widehat \\pi_k$. Report the performance of your classifier on the test set of 1797 digits in `optdigits.tes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Solution:**\n",
    "\n",
    "From the training set, we get the prior probability $\\widehat \\pi_k$, stored in matrix $\\alpha_{all}$. For each data in the test set, we need to fit the data point with each of the digit model with the training MLE $\\alpha$. From the 10 loglikelihoods for fitting each digit model, we can find the one with the greatest loglikelihood. This means the data represents the corresponding digit with the greatest opportunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# readin test data\n",
    "# read in train data\n",
    "path1_test = \"http://hua-zhou.github.io/teaching/biostatm280\"\n",
    "path2_test = \"-2018spring/hw/hw4/optdigits.tes\"\n",
    "\n",
    "test = readdlm(download(path1_test * path2_test), ',');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the training data set to Int64\n",
    "X = round.(Int64, test);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "α[j] + x[j] = 0.0\n",
      "lgamma(α[j] + x[j]) = Inf\n",
      "logden = NaN\n"
     ]
    }
   ],
   "source": [
    "# convert the training data set to Int64\n",
    "X = round.(Int64, test);\n",
    "\n",
    "# separte last column which store the digit \n",
    "test_x = (round.(Int64, X[:, 1:64]))’\n",
    "test_digit = round.(Int64, X[:, 65]);\n",
    "\n",
    "# prediction matrix\n",
    "pred = zeros(size(test_x, 2))\n",
    "\n",
    "    # get the train_x subset for different digits\n",
    "a = [dirmult_logpdf(X[:, j], α_all[:, d]) for d in 1:10, \n",
    "        j in 1:size(X, 2)]\n",
    "x = X[:, 1]\n",
    "α=α_all[:, 2]\n",
    "\n",
    "for j in 1:size(test_x, 2)\n",
    "    for digit in 0:9\n",
    "         \n",
    "    end\n",
    "end\n",
    "\n",
    "# remove 0s in α and the corresponding elements in x\n",
    "row_ind = find(α_all[:, (digit + 1)])\n",
    "\n",
    "    # store the sum of x and α\n",
    "    sumx = sum(x)\n",
    "    sumα = sum(α)\n",
    "    \n",
    "    # store length of x\n",
    "    lengthx = length(x) \n",
    "    \n",
    "    # part without summation\n",
    "    logden = lfact(sumx) - lgamma(sumα + sumx) + lgamma(sumα)\n",
    "    \n",
    "    # part within summation\n",
    "    for j in 1:lengthx\n",
    "        @show α[j] + x[j]\n",
    "        @show lgamma(α[j] + x[j])\n",
    "        logden += lgamma(α[j] + x[j]) - lgamma(α[j]) - lfact(x[j])  \n",
    "        @show logden\n",
    "    end\n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
